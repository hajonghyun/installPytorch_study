{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQrjTaf/2oPWwme0Ni3B6f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hajonghyun/installPytorch_study/blob/main/1_torch_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xdvTxOWPE3S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.tensor` vs `np.array` 핵심 비교\n",
        "\n",
        "AI 엔지니어링, 특히 딥러닝 학습 단계로 넘어갈 때 가장 명확하게 이해해야 하는 두 자료형의 차이점입니다.\n",
        "\n",
        "### 1. 요약 표\n",
        "\n",
        "| 비교 항목 | NumPy (`np.array`) | PyTorch (`torch.tensor`) |\n",
        "| :--- | :--- | :--- |\n",
        "| **주 용도** | 일반 수치 계산, 데이터 전처리, 머신러닝(Scikit-learn) | **딥러닝 모델 학습**, GPU 가속 연산 |\n",
        "| **하드웨어** | **CPU** 연산만 가능 | **CPU + GPU** 연산 지원 (`.to('cuda')`) |\n",
        "| **미분(Gradient)** | 지원 안 함 (수식 직접 구현 필요) | **Autograd** (자동 미분) 지원 (`requires_grad=True`) |\n",
        "| **메모리 공유** | `from_numpy()` 사용 시 메모리 공유 가능 | `torch.as_tensor()` 등으로 효율적 변환 가능 |\n",
        "\n",
        "---\n",
        "\n",
        "### 2. 주요 차이점 상세\n",
        "\n",
        "#### A. GPU 가속 (Hardware Acceleration)\n",
        "**가장 큰 차이점**입니다. NumPy는 CPU에서만 작동하지만, Tensor는 GPU로 데이터를 옮겨 대규모 병렬 연산(행렬 곱 등)을 빠르게 처리할 수 있습니다.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# NumPy: 오직 CPU 메모리 사용\n",
        "arr = np.array([1, 2, 3])\n",
        "\n",
        "# PyTorch: GPU로 이동 가능\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "if torch.cuda.is_available():\n",
        "    tensor = tensor.to('cuda') # 데이터를 GPU VRAM에 올림\n",
        "```\n",
        "\n",
        "#### B. 자동 미분 (Autograd)\n",
        "딥러닝의 핵심인 **역전파(Backpropagation)**를 수행하기 위해, PyTorch Tensor는 연산의 히스토리를 추적하고 미분값을 저장할 수 있습니다.\n",
        "\n",
        "```python\n",
        "# requires_grad=True: 이 텐서에 대한 연산을 추적하겠다는 의미\n",
        "w = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "y = w ** 2      # 수식: y = w^2\n",
        "y.backward()    # 미분 수행 (dy/dw)\n",
        "\n",
        "print(w.grad)   # 결과: 4.0 (2 * w)\n",
        "```\n",
        "* **NumPy**는 단순히 값을 저장하고 계산할 뿐, 미분 계수(`grad`)를 자동으로 계산해주지 않습니다.\n",
        "\n",
        "### 3. 결론 (AI 엔지니어 관점)\n",
        "* **데이터 전처리/분석 단계:** `np.array`와 Pandas를 주로 사용합니다.\n",
        "* **모델 학습/추론 단계:** 데이터를 `torch.tensor`로 변환하여 GPU에 올리고 모델에 주입합니다."
      ],
      "metadata": {
        "id": "OqE9dGFdSzCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([1,2,3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJw-CN7_PdhT",
        "outputId": "638efbfa-d27b-4a4d-8730-e67b79e82b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([1,2,3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih9waJeiPkDs",
        "outputId": "37035b35-4c6f-49e4-e3b8-22ee5a718e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `type()` vs `.dtype` 차이점 정리\n",
        "\n",
        "파이썬으로 데이터 사이언스나 딥러닝을 할 때, **디버깅(에러 해결)의 80%는 이 두 가지를 구분하는 것**에서 시작됩니다.\n",
        "\n",
        "### 1. 핵심 요약\n",
        "\n",
        "| 구분 | 명령어 예시 | 설명 | 비유 (컵과 내용물) |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **`type()`** | `type(data)` | **객체(컨테이너) 자체의 자료형**을 확인합니다.<br>(예: 이것은 리스트인가? 텐서인가?) | **\"이 컵은 유리컵인가, 머그컵인가?\"** |\n",
        "| **`.dtype`** | `data.dtype` | **객체 안에 담긴 데이터(원소)의 자료형**을 확인합니다.<br>(예: 텐서 안에 들어있는 숫자가 `int`인가 `float`인가?) | **\"컵 안에 든 것이 물인가, 콜라인가?\"** |\n",
        "\n",
        "---\n",
        "\n",
        "### 2. 코드 예시 (PyTorch & NumPy)\n",
        "\n",
        "가장 흔히 혼동하는 상황을 코드로 확인해 보세요.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 데이터 생성\n",
        "np_arr = np.array([1.0, 2.0, 3.0])\n",
        "torch_tensor = torch.tensor([1, 2, 3]) # 정수로 생성\n",
        "\n",
        "# 1. type(): 껍데기 확인\n",
        "print(type(np_arr))       # <class 'numpy.ndarray'> -> \"이것은 넘파이 배열입니다.\"\n",
        "print(type(torch_tensor)) # <class 'torch.Tensor'>  -> \"이것은 파이토치 텐서입니다.\"\n",
        "\n",
        "# 2. .dtype: 내용물 확인 (매우 중요!)\n",
        "print(np_arr.dtype)       # float64 -> \"안에 실수가 들어있습니다.\"\n",
        "print(torch_tensor.dtype) # torch.int64 -> \"안에 정수가 들어있습니다.\"\n",
        "```\n",
        "\n",
        "### 3. 왜 중요한가요? (AI 엔지니어 관점)\n",
        "\n",
        "* **`type()` 에러:** 주로 **호환성** 문제입니다.\n",
        "    * *예: PyTorch 모델에 실수로 NumPy 배열을 넣으면 `TypeError`가 발생합니다.*\n",
        "* **`.dtype` 에러:** 주로 **정밀도나 연산** 문제입니다.\n",
        "    * *예: 딥러닝 모델 가중치는 보통 `float32`인데, 입력 데이터가 `int64`나 `float64`면 `RuntimeError: expected scalar type Float but found Double` 같은 에러가 뜹니다.*\n",
        "    * **해결:** `tensor.to(torch.float32)` 처럼 캐스팅(형변환)을 해줘야 합니다."
      ],
      "metadata": {
        "id": "OioxCeBwTwrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2,3])\n",
        "b = torch.tensor([1.0, 2, 3]) # 하나라도 실수면 dtype은 float"
      ],
      "metadata": {
        "id": "J7Hv3GNORlPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(a), type(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUnd5I7mPnST",
        "outputId": "50c5622f-1c37-446c-a17d-9dbe5faeed40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.dtype, b.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gngi16aNPs4z",
        "outputId": "2d9d43d4-caf8-4c2d-dd35-42dd36c5ff7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64 torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9dhoVsHRz-u",
        "outputId": "4773c6ff-e9de-41d5-a898-7eb9e01d57cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor 핵심 속성 및 형태 규칙\n",
        "\n",
        "### 1. 형태 규칙 (Shape Constraint)\n",
        "* **직사각형 형태 강제 (Strict Rectangular):** Tensor는 GPU 병렬 연산을 위해 모든 행(Row)의 길이가 반드시 같아야 합니다.\n",
        "    * `[[1, 2, 3], [4, 5]]` ❌ : **Jagged Tensor 불가** (에러 발생)\n",
        "    * `[[1, 2, 3], [4, 5, 6]]` ⭕ : 생성 가능\n",
        "\n",
        "### 2. 필수 속성 조회 3대장\n",
        "\n",
        "딥러닝 모델 디버깅 시 `print()`로 가장 많이 찍어보는 3가지입니다.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "a = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "\n",
        "# 1. 모양 (Shape) - ★가장 중요★\n",
        "# 각 차원의 크기를 확인 (행, 열)\n",
        "print(a.shape)    # torch.Size([2, 3])\n",
        "\n",
        "# 2. 차원 수 (Dimension/Rank)\n",
        "# 몇 차원 텐서인지 확인\n",
        "print(a.ndim)     # 2\n",
        "\n",
        "# 3. 전체 원소 개수 (Number of Elements)\n",
        "# 2행 * 3열 = 총 6개\n",
        "print(a.numel())  # 6\n",
        "```"
      ],
      "metadata": {
        "id": "tnVZp4OAUIYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1,2,3],\n",
        "                  [4,5,6]])\n",
        "# b = torch.tensor([[1,2,3],\n",
        "#                   [4,5]])\n",
        "# np.array와는 달리 torch.tensor는 행렬\n",
        "# 각 행에 해당하는 숫자의 개수 같아야함.\n",
        "\n",
        "print(a.shape)\n",
        "print(a.ndim) # 차원 수\n",
        "print(a.numel()) # num of element 요소의 수"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGWGyPCRT5m2",
        "outputId": "1c9c568f-5981-4d78-a50f-f3ef97586a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n",
            "2\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor 생성 및 초기화 (NumPy와 비교)\n",
        "\n",
        "PyTorch는 NumPy와 매우 유사한 API를 제공하지만, **Shape 입력 방식**에서 더 유연합니다.\n",
        "\n",
        "### 1. 0 또는 1로 채우기 (`zeros`, `ones`)\n",
        "* **핵심 차이:** `torch`는 차원(Shape)을 튜플 `()`로 묶지 않고 **인자로 바로 나열**해도 됩니다. (NumPy는 튜플 필수)\n",
        "\n",
        "```python\n",
        "# 5행 5열 0행렬\n",
        "print(torch.zeros(5, 5))  # OK: 괄호 하나로 충분\n",
        "# print(np.zeros(5, 5))   # Error: np.zeros((5, 5))여야 함\n",
        "\n",
        "# 2행 2열 1행렬\n",
        "print(torch.ones(2, 2))\n",
        "```\n",
        "\n",
        "### 2. 형태 복제하기 (`_like`)\n",
        "* 기존 텐서 `a`의 **Shape(모양), Dtype(자료형), Device(CPU/GPU)** 속성을 그대로 물려받아 새로운 텐서를 만듭니다.\n",
        "\n",
        "```python\n",
        "# a와 똑같은 모양으로 0을 채움\n",
        "print(torch.zeros_like(a))\n",
        "```\n",
        "\n",
        "### 3. 수열 생성 (`arange`, `linspace`)\n",
        "* 데이터 인덱싱이나 그래프 축(x-axis)을 만들 때 주로 사용합니다.\n",
        "\n",
        "| 함수 | 설명 | 인자 의미 | 예시 |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **`arange`** | 간격(Step) 기준 생성 | `(시작, 끝, 간격)` | `torch.arange(0, 10, 2)`<br>→ `[0, 2, 4, 6, 8]` (끝 미포함) |\n",
        "| **`linspace`** | 개수(Count) 기준 생성 | `(시작, 끝, 개수)` | `torch.linspace(0, 10, 5)`<br>→ `[0, 2.5, 5, 7.5, 10]` (끝 포함) |"
      ],
      "metadata": {
        "id": "wGdR5YwuXLXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.zeros(5,5))\n",
        "print(np.zeros((5,5)))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(torch.zeros_like(a))\n",
        "print(np.zeros_like(a))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(torch.ones(2,2))\n",
        "print(np.ones((2,2)))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(torch.arange(3,10,2))\n",
        "print(np.arange(3,10,2))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(torch.linspace(1,10,10))\n",
        "print(np.linspace(1,10,10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zxxNGlSUR9N",
        "outputId": "19b033ca-154a-4967-e44a-efe532fdd4c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n",
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0]])\n",
            "[[0 0 0]\n",
            " [0 0 0]]\n",
            "\n",
            "\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "[[1. 1.]\n",
            " [1. 1.]]\n",
            "\n",
            "\n",
            "tensor([3, 5, 7, 9])\n",
            "[3 5 7 9]\n",
            "\n",
            "\n",
            "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
            "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor 기본 연산 (Element-wise vs Dot Product)\n",
        "\n",
        "PyTorch 연산에서 가장 중요한 것은 **`*` (단순 곱)**과 **`@` (행렬 곱)**을 구분하는 것입니다.\n",
        "\n",
        "### 1. 원소별 연산 (Element-wise)\n",
        "기본 산술 연산자(`+`, `-`, `*`, `/`, `**`)는 **같은 위치(Index)에 있는 원소끼리** 1:1로 계산합니다. 결과의 모양(Shape)이 유지됩니다.\n",
        "\n",
        "```python\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([3, 2, 1])\n",
        "\n",
        "# [1*3, 2*2, 3*1]\n",
        "print(a * b)   # tensor([3, 4, 3]) -> 이를 아다마르 곱(Hadamard Product)이라고도 함\n",
        "\n",
        "# [1^2, 2^2, 3^2]\n",
        "print(a ** 2)  # tensor([1, 4, 9])\n",
        "```\n",
        "\n",
        "### 2. 내적 / 행렬 곱 (Dot Product)\n",
        "**`@` 연산자**는 벡터의 내적(Inner Product) 또는 행렬 곱셈을 수행합니다. 1차원 벡터끼리 연산하면 결과가 하나의 값(Scalar)으로 합쳐집니다.\n",
        "\n",
        "```python\n",
        "# 계산 과정: (1*3) + (2*2) + (3*1) = 3 + 4 + 3 = 10\n",
        "print(a @ b)   # tensor(10)\n",
        "```\n",
        "\n",
        "> **Tip:** 과거에는 `torch.matmul(a, b)`를 많이 썼지만, 최신 코드에서는 가독성을 위해 **`a @ b`**를 더 선호합니다."
      ],
      "metadata": {
        "id": "CegvZSWpYwHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2,3])\n",
        "b = torch.tensor([3,2,1])\n",
        "print(a+b)\n",
        "print(a*b) #\n",
        "print(a@b) #\n",
        "\n",
        "print(a/b)\n",
        "print(a**2)  # 제곱도 각 성분에 대해서."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XaR94r-YANI",
        "outputId": "d5c6147b-ac7c-4835-af6c-18a765f0267d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4, 4, 4])\n",
            "tensor([3, 4, 3])\n",
            "tensor(10)\n",
            "tensor([0.3333, 1.0000, 3.0000])\n",
            "tensor([1, 4, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pytorch의 인덱싱과 슬라이싱\n",
        "--> numpy, list와 똑같음."
      ],
      "metadata": {
        "id": "1_ng9DjMZ3vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2,3,4,5,6,7,8,9])\n",
        "\n",
        "print(a[7:])\n",
        "print(a[2:5])\n",
        "print(a[::2])\n",
        "print(a[:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0p3bJVnumGU",
        "outputId": "d4da7d40-24e4-4ce1-e8e4-300a102e6d72"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([8, 9])\n",
            "tensor([3, 4, 5])\n",
            "tensor([1, 3, 5, 7, 9])\n",
            "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1,2,3],\n",
        "                  [4,5,6],\n",
        "                  [7,8,9]])\n",
        "print(a[0])\n",
        "print(a[-1])\n",
        "print(a[1:])\n",
        "print(a[:])\n",
        "print(a[0][2])\n",
        "print(a[0,2])   # 일반 list에서는 불가! np, torch에서만 가능\n",
        "\n",
        "print(a[:][2]) # a[:] == a\n",
        "print(a[:,2]) # 위와 결과가 다름을 알 수 있음.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gR_2Bd-0ut_0",
        "outputId": "2bcc20cb-cbf8-4609-83cb-b072bf3ccf3e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "tensor([7, 8, 9])\n",
            "tensor([[4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "tensor(3)\n",
            "tensor(3)\n",
            "tensor([7, 8, 9])\n",
            "tensor([3, 6, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3차원 텐서와 차원(Shape) 규칙\n",
        "\n",
        "### 1. 3차원 텐서 구조 해석 (`2, 3, 4`)\n",
        "텐서의 Shape은 **가장 바깥쪽 대괄호부터 안쪽으로** 파고들며 해석합니다.\n",
        "\n",
        "* `torch.Size([2, 3, 4])`\n",
        "    * **2 (Depth):** 큰 덩어리(면)가 2개\n",
        "    * **3 (Row):** 각 덩어리 안에 행이 3개\n",
        "    * **4 (Col):** 각 행 안에 열(원소)이 4개\n",
        "\n",
        "### 2. 대괄호와 차원의 관계 (The Bracket Rule)\n",
        "**\"대괄호 `[]`를 한 겹 씌울 때마다, Shape의 맨 왼쪽에 `1`이 추가됩니다.\"**\n",
        "\n",
        "내용물(데이터)은 같아도, 대괄호로 감싸면 **차원(Rank)**이 높아집니다.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# 1. 기본: 1차원 벡터 (요소 4개)\n",
        "a = torch.tensor([1, 2, 3, 4])\n",
        "print(a.shape) # torch.Size([4])\n",
        "\n",
        "# 2. 대괄호 1겹 추가 -> 2차원 (1행 4열)\n",
        "b = torch.tensor([[1, 2, 3, 4]])\n",
        "print(b.shape) # torch.Size([1, 4])\n",
        "\n",
        "# 3. 대괄호 2겹 추가 -> 3차원 (덩어리 1개, 1행 4열)\n",
        "c = torch.tensor([[[1, 2, 3, 4]]])\n",
        "print(c.shape) # torch.Size([1, 1, 4])\n",
        "\n",
        "# 4. 대괄호 3겹 추가 -> 4차원\n",
        "d = torch.tensor([[[[1, 2, 3, 4]]]])\n",
        "print(d.shape) # torch.Size([1, 1, 1, 4])\n",
        "```\n",
        "\n",
        "> **Tip:** 이렇게 `1`인 차원이 불필요하게 생겼을 때는 **`squeeze()`** 함수로 제거할 수 있습니다."
      ],
      "metadata": {
        "id": "cqE-ObBlxJ1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3차원 행렬 인덱싱\n",
        "a = torch.tensor([[[0,1,2,3],[4,5,6,7],[8,9,10,11]],\n",
        "                  [[12,13,14,15],[16,17,18,19],[20,21,22,23]]])\n",
        "print(a)\n",
        "print(a.shape)\n",
        "print(a.ndim)\n",
        "\n",
        "\n",
        "# 대괄호가 하나 늘어나면 왼쪽에 shape값이 추가된다.\n",
        "a = torch.tensor([[1,2,3,4]])\n",
        "print(a.shape)\n",
        "\n",
        "a = torch.tensor([[[1,2,3,4]]])\n",
        "print(a.shape)\n",
        "\n",
        "a = torch.tensor([[[[1,2,3,4]]]])\n",
        "print(a.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io3XLmSRvo1U",
        "outputId": "0fdf61be-0801-4306-92e9-36789c85bfc6"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0,  1,  2,  3],\n",
            "         [ 4,  5,  6,  7],\n",
            "         [ 8,  9, 10, 11]],\n",
            "\n",
            "        [[12, 13, 14, 15],\n",
            "         [16, 17, 18, 19],\n",
            "         [20, 21, 22, 23]]])\n",
            "torch.Size([2, 3, 4])\n",
            "3\n",
            "torch.Size([1, 4])\n",
            "torch.Size([1, 1, 4])\n",
            "torch.Size([1, 1, 1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch 팬시 인덱싱 (Fancy Indexing)\n",
        "\n",
        "슬라이싱(`:`)이 범위를 칼로 자르는 것이라면, 팬시 인덱싱은 **원하는 지점만 콕콕 집어내거나(Picking)** 순서를 내 마음대로 섞는 기법입니다.\n",
        "\n",
        "## 1. 판단 기준: \"이게 팬시 인덱싱인가?\"\n",
        "대괄호 `[]` 안에 **숫자 하나**나 **슬라이싱(`:`)**이 아닌, **또 다른 리스트(List)나 배열(Tensor)**이 들어가 있다면 팬시 인덱싱입니다.\n",
        "\n",
        "* `a[1:3]` : 슬라이싱 (범위)\n",
        "* `a[[1, 3]]` : **팬시 인덱싱** (1번과 3번만 선택)\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 핵심 해석법: \"메인 콤마(,)의 법칙\" ★★★\n",
        "복잡한 대괄호 속에서 길을 잃지 않는 유일한 법칙입니다. **가장 바깥쪽 콤마**가 차원을 가르는 국경선입니다.\n",
        "\n",
        "### Case A. 콤마가 있다? -> `a[ [0,1], [0,1] ]`\n",
        "* **의미:** 차원별로 좌표를 따로 줬다는 뜻입니다.\n",
        "* **해석법:** **\"좌표 찍기 (Zipping)\"**\n",
        "    * 리스트들을 세로로 나란히 놓고 같은 순서끼리 묶어서 $(x, y)$ 좌표를 만듭니다.\n",
        "    * 예: `(0,0)` 좌표의 값 하나, `(1,1)` 좌표의 값 하나.\n",
        "\n",
        "### Case B. 콤마가 없다? -> `a[ [0,1] ]`\n",
        "* **의미:** 리스트 전체가 **첫 번째 차원(Dim 0)**에 통째로 들어갔다는 뜻입니다.\n",
        "* **해석법:** **\"덩어리 선택 (Selection)\"**\n",
        "    * \"0번 덩어리와 1번 덩어리를 통째로 가져와라.\"\n",
        "    * 남은 차원은 건드리지 않았으므로 그대로 유지됩니다."
      ],
      "metadata": {
        "id": "w8gS9l8OyK41"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8A8T4XMB6SKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDS8LX3v6SIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[[0,1,2,3],[4,5,6,7],[8,9,10,11]],\n",
        "                  [[12,13,14,15],[16,17,18,19],[20,21,22,23]]])\n",
        "print(a)\n",
        "a[[0,1,1,0],[0,1,2,1],[3,3,2,1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmORxLfAyLCc",
        "outputId": "7f764708-c4f9-4f61-adc6-9a182c2fc44f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0,  1,  2,  3],\n",
            "         [ 4,  5,  6,  7],\n",
            "         [ 8,  9, 10, 11]],\n",
            "\n",
            "        [[12, 13, 14, 15],\n",
            "         [16, 17, 18, 19],\n",
            "         [20, 21, 22, 23]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3, 19, 22,  5])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 팬시 인덱싱 점검: 4차원 텐서 퀴즈\n",
        "\n",
        "제대로 이해했는지 확인하는 고난도 4차원 문제입니다.\n",
        "\n",
        "### 기본 설정\n",
        "0부터 15까지 숫자가 채워진 `(2, 2, 2, 2)` 텐서입니다.\n",
        "```python\n",
        "import torch\n",
        "a = torch.arange(16).view(2, 2, 2, 2)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "o4HL1AkR6lvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shape: (2, 2, 2, 2) -> (B, C, H, W)라고 상상해보세요.\n",
        "a = torch.arange(16).view(2, 2, 2, 2)\n",
        "\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0gEtZlM3QqK",
        "outputId": "6b1735f8-d8db-4f4d-de9c-2542e2389f9d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0,  1],\n",
            "          [ 2,  3]],\n",
            "\n",
            "         [[ 4,  5],\n",
            "          [ 6,  7]]],\n",
            "\n",
            "\n",
            "        [[[ 8,  9],\n",
            "          [10, 11]],\n",
            "\n",
            "         [[12, 13],\n",
            "          [14, 15]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. 좌표 콕콕 찍기 (Basic)\n",
        "리스트가 모든 차원에 다 들어간 경우입니다.\n",
        "```python\n",
        "# 힌트: 세로로 묶어서 좌표(0,0,0,0)과 (1,1,1,1)을 만드세요.\n",
        "print(a[[0, 1], [0, 1], [0, 1], [0, 1]])\n",
        "```\n",
        "> **정답:** `tensor([0, 15])`\n",
        "> **해설:** 점 2개를 핀셋으로 집어낸 결과이므로 1차원 벡터가 됩니다."
      ],
      "metadata": {
        "id": "qQc25lAF6upp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a[[0, 1], [0, 1], [0, 1], [0, 1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9E4O1mB3Rc0",
        "outputId": "91348668-8b21-46b7-8b1d-8de35be94b91"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. 고정과 선택의 조화 (Intermediate)\n",
        "앞쪽 차원은 숫자로 고정하고, 뒤쪽 차원만 리스트로 선택하는 경우입니다.\n",
        "```python\n",
        "# 힌트: 앞의 0, 1은 고정! 뒤의 [0,1], [1,0]만 짝을 지으세요.\n",
        "print(a[0, 1, [0, 1], [1, 0]])\n",
        "```\n",
        "> **정답:** `tensor([5, 6])`\n",
        "> **해설:**\n",
        "> * 좌표 1: `(0, 1, 0, 1)` -> 값 5\n",
        "> * 좌표 2: `(0, 1, 1, 0)` -> 값 6"
      ],
      "metadata": {
        "id": "p-Qw3lZh60uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a[0, 1, [0, 1], [1, 0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTb4Xg5v4FPF",
        "outputId": "318e2c40-58d0-4c16-cfe8-792c0d3f06b8"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. 덩어리째 가져오기 (Advanced)\n",
        "**메인 콤마가 없는** 경우입니다.\n",
        "```python\n",
        "# 힌트: [1, 1, 0] 리스트 전체가 Dim 0에 적용됩니다.\n",
        "print(a[[1, 1, 0]].shape)\n",
        "```\n",
        "> **정답:** `torch.Size([3, 2, 2, 2])`\n",
        "> **해설:**\n",
        "> * Dim 0에서 1번, 1번, 0번 덩어리를 순서대로 가져왔으므로 개수가 **3개**가 됩니다.\n",
        "> * 뒤쪽 차원 `(2, 2, 2)`는 건드리지 않았으므로 그대로 유지됩니다."
      ],
      "metadata": {
        "id": "vQy3iwVw63OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a[[1, 1, 0]].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gma6U8ZA40xJ",
        "outputId": "139ff5dd-3f66-48b1-e8c4-d3a16f9c9136"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2, 2, 2])\n"
          ]
        }
      ]
    }
  ]
}