{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj3M3Ku5G+QuoHq14febQC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hajonghyun/installPytorch_study/blob/main/1_torch_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xdvTxOWPE3S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.tensor` vs `np.array` 핵심 비교\n",
        "\n",
        "AI 엔지니어링, 특히 딥러닝 학습 단계로 넘어갈 때 가장 명확하게 이해해야 하는 두 자료형의 차이점입니다.\n",
        "\n",
        "### 1. 요약 표\n",
        "\n",
        "| 비교 항목 | NumPy (`np.array`) | PyTorch (`torch.tensor`) |\n",
        "| :--- | :--- | :--- |\n",
        "| **주 용도** | 일반 수치 계산, 데이터 전처리, 머신러닝(Scikit-learn) | **딥러닝 모델 학습**, GPU 가속 연산 |\n",
        "| **하드웨어** | **CPU** 연산만 가능 | **CPU + GPU** 연산 지원 (`.to('cuda')`) |\n",
        "| **미분(Gradient)** | 지원 안 함 (수식 직접 구현 필요) | **Autograd** (자동 미분) 지원 (`requires_grad=True`) |\n",
        "| **메모리 공유** | `from_numpy()` 사용 시 메모리 공유 가능 | `torch.as_tensor()` 등으로 효율적 변환 가능 |\n",
        "\n",
        "---\n",
        "\n",
        "### 2. 주요 차이점 상세\n",
        "\n",
        "#### A. GPU 가속 (Hardware Acceleration)\n",
        "**가장 큰 차이점**입니다. NumPy는 CPU에서만 작동하지만, Tensor는 GPU로 데이터를 옮겨 대규모 병렬 연산(행렬 곱 등)을 빠르게 처리할 수 있습니다.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# NumPy: 오직 CPU 메모리 사용\n",
        "arr = np.array([1, 2, 3])\n",
        "\n",
        "# PyTorch: GPU로 이동 가능\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "if torch.cuda.is_available():\n",
        "    tensor = tensor.to('cuda') # 데이터를 GPU VRAM에 올림\n",
        "```\n",
        "\n",
        "#### B. 자동 미분 (Autograd)\n",
        "딥러닝의 핵심인 **역전파(Backpropagation)**를 수행하기 위해, PyTorch Tensor는 연산의 히스토리를 추적하고 미분값을 저장할 수 있습니다.\n",
        "\n",
        "```python\n",
        "# requires_grad=True: 이 텐서에 대한 연산을 추적하겠다는 의미\n",
        "w = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "y = w ** 2      # 수식: y = w^2\n",
        "y.backward()    # 미분 수행 (dy/dw)\n",
        "\n",
        "print(w.grad)   # 결과: 4.0 (2 * w)\n",
        "```\n",
        "* **NumPy**는 단순히 값을 저장하고 계산할 뿐, 미분 계수(`grad`)를 자동으로 계산해주지 않습니다.\n",
        "\n",
        "### 3. 결론 (AI 엔지니어 관점)\n",
        "* **데이터 전처리/분석 단계:** `np.array`와 Pandas를 주로 사용합니다.\n",
        "* **모델 학습/추론 단계:** 데이터를 `torch.tensor`로 변환하여 GPU에 올리고 모델에 주입합니다."
      ],
      "metadata": {
        "id": "OqE9dGFdSzCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([1,2,3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJw-CN7_PdhT",
        "outputId": "638efbfa-d27b-4a4d-8730-e67b79e82b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([1,2,3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih9waJeiPkDs",
        "outputId": "37035b35-4c6f-49e4-e3b8-22ee5a718e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `type()` vs `.dtype` 차이점 정리\n",
        "\n",
        "파이썬으로 데이터 사이언스나 딥러닝을 할 때, **디버깅(에러 해결)의 80%는 이 두 가지를 구분하는 것**에서 시작됩니다.\n",
        "\n",
        "### 1. 핵심 요약\n",
        "\n",
        "| 구분 | 명령어 예시 | 설명 | 비유 (컵과 내용물) |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **`type()`** | `type(data)` | **객체(컨테이너) 자체의 자료형**을 확인합니다.<br>(예: 이것은 리스트인가? 텐서인가?) | **\"이 컵은 유리컵인가, 머그컵인가?\"** |\n",
        "| **`.dtype`** | `data.dtype` | **객체 안에 담긴 데이터(원소)의 자료형**을 확인합니다.<br>(예: 텐서 안에 들어있는 숫자가 `int`인가 `float`인가?) | **\"컵 안에 든 것이 물인가, 콜라인가?\"** |\n",
        "\n",
        "---\n",
        "\n",
        "### 2. 코드 예시 (PyTorch & NumPy)\n",
        "\n",
        "가장 흔히 혼동하는 상황을 코드로 확인해 보세요.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 데이터 생성\n",
        "np_arr = np.array([1.0, 2.0, 3.0])\n",
        "torch_tensor = torch.tensor([1, 2, 3]) # 정수로 생성\n",
        "\n",
        "# 1. type(): 껍데기 확인\n",
        "print(type(np_arr))       # <class 'numpy.ndarray'> -> \"이것은 넘파이 배열입니다.\"\n",
        "print(type(torch_tensor)) # <class 'torch.Tensor'>  -> \"이것은 파이토치 텐서입니다.\"\n",
        "\n",
        "# 2. .dtype: 내용물 확인 (매우 중요!)\n",
        "print(np_arr.dtype)       # float64 -> \"안에 실수가 들어있습니다.\"\n",
        "print(torch_tensor.dtype) # torch.int64 -> \"안에 정수가 들어있습니다.\"\n",
        "```\n",
        "\n",
        "### 3. 왜 중요한가요? (AI 엔지니어 관점)\n",
        "\n",
        "* **`type()` 에러:** 주로 **호환성** 문제입니다.\n",
        "    * *예: PyTorch 모델에 실수로 NumPy 배열을 넣으면 `TypeError`가 발생합니다.*\n",
        "* **`.dtype` 에러:** 주로 **정밀도나 연산** 문제입니다.\n",
        "    * *예: 딥러닝 모델 가중치는 보통 `float32`인데, 입력 데이터가 `int64`나 `float64`면 `RuntimeError: expected scalar type Float but found Double` 같은 에러가 뜹니다.*\n",
        "    * **해결:** `tensor.to(torch.float32)` 처럼 캐스팅(형변환)을 해줘야 합니다."
      ],
      "metadata": {
        "id": "OioxCeBwTwrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2,3])\n",
        "b = torch.tensor([1.0, 2, 3]) # 하나라도 실수면 dtype은 float"
      ],
      "metadata": {
        "id": "J7Hv3GNORlPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(a), type(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUnd5I7mPnST",
        "outputId": "50c5622f-1c37-446c-a17d-9dbe5faeed40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.dtype, b.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gngi16aNPs4z",
        "outputId": "2d9d43d4-caf8-4c2d-dd35-42dd36c5ff7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64 torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9dhoVsHRz-u",
        "outputId": "4773c6ff-e9de-41d5-a898-7eb9e01d57cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor 핵심 속성 및 형태 규칙\n",
        "\n",
        "### 1. 형태 규칙 (Shape Constraint)\n",
        "* **직사각형 형태 강제 (Strict Rectangular):** Tensor는 GPU 병렬 연산을 위해 모든 행(Row)의 길이가 반드시 같아야 합니다.\n",
        "    * `[[1, 2, 3], [4, 5]]` ❌ : **Jagged Tensor 불가** (에러 발생)\n",
        "    * `[[1, 2, 3], [4, 5, 6]]` ⭕ : 생성 가능\n",
        "\n",
        "### 2. 필수 속성 조회 3대장\n",
        "\n",
        "딥러닝 모델 디버깅 시 `print()`로 가장 많이 찍어보는 3가지입니다.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "a = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "\n",
        "# 1. 모양 (Shape) - ★가장 중요★\n",
        "# 각 차원의 크기를 확인 (행, 열)\n",
        "print(a.shape)    # torch.Size([2, 3])\n",
        "\n",
        "# 2. 차원 수 (Dimension/Rank)\n",
        "# 몇 차원 텐서인지 확인\n",
        "print(a.ndim)     # 2\n",
        "\n",
        "# 3. 전체 원소 개수 (Number of Elements)\n",
        "# 2행 * 3열 = 총 6개\n",
        "print(a.numel())  # 6\n",
        "```"
      ],
      "metadata": {
        "id": "tnVZp4OAUIYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1,2,3],\n",
        "                  [4,5,6]])\n",
        "# b = torch.tensor([[1,2,3],\n",
        "#                   [4,5]])\n",
        "# np.array와는 달리 torch.tensor는 행렬\n",
        "# 각 행에 해당하는 숫자의 개수 같아야함.\n",
        "\n",
        "print(a.shape)\n",
        "print(a.ndim) # 차원 수\n",
        "print(a.numel()) # num of element 요소의 수"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGWGyPCRT5m2",
        "outputId": "1c9c568f-5981-4d78-a50f-f3ef97586a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n",
            "2\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor 생성 및 초기화 (NumPy와 비교)\n",
        "\n",
        "PyTorch는 NumPy와 매우 유사한 API를 제공하지만, **Shape 입력 방식**에서 더 유연합니다.\n",
        "\n",
        "### 1. 0 또는 1로 채우기 (`zeros`, `ones`)\n",
        "* **핵심 차이:** `torch`는 차원(Shape)을 튜플 `()`로 묶지 않고 **인자로 바로 나열**해도 됩니다. (NumPy는 튜플 필수)\n",
        "\n",
        "```python\n",
        "# 5행 5열 0행렬\n",
        "print(torch.zeros(5, 5))  # OK: 괄호 하나로 충분\n",
        "# print(np.zeros(5, 5))   # Error: np.zeros((5, 5))여야 함\n",
        "\n",
        "# 2행 2열 1행렬\n",
        "print(torch.ones(2, 2))\n",
        "```\n",
        "\n",
        "### 2. 형태 복제하기 (`_like`)\n",
        "* 기존 텐서 `a`의 **Shape(모양), Dtype(자료형), Device(CPU/GPU)** 속성을 그대로 물려받아 새로운 텐서를 만듭니다.\n",
        "\n",
        "```python\n",
        "# a와 똑같은 모양으로 0을 채움\n",
        "print(torch.zeros_like(a))\n",
        "```\n",
        "\n",
        "### 3. 수열 생성 (`arange`, `linspace`)\n",
        "* 데이터 인덱싱이나 그래프 축(x-axis)을 만들 때 주로 사용합니다.\n",
        "\n",
        "| 함수 | 설명 | 인자 의미 | 예시 |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **`arange`** | 간격(Step) 기준 생성 | `(시작, 끝, 간격)` | `torch.arange(0, 10, 2)`<br>→ `[0, 2, 4, 6, 8]` (끝 미포함) |\n",
        "| **`linspace`** | 개수(Count) 기준 생성 | `(시작, 끝, 개수)` | `torch.linspace(0, 10, 5)`<br>→ `[0, 2.5, 5, 7.5, 10]` (끝 포함) |"
      ],
      "metadata": {
        "id": "wGdR5YwuXLXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.zeros(5,5))\n",
        "print(np.zeros((5,5)))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(torch.zeros_like(a))\n",
        "print(np.zeros_like(a))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(torch.ones(2,2))\n",
        "print(np.ones((2,2)))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(torch.arange(3,10,2))\n",
        "print(np.arange(3,10,2))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(torch.linspace(1,10,10))\n",
        "print(np.linspace(1,10,10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zxxNGlSUR9N",
        "outputId": "19b033ca-154a-4967-e44a-efe532fdd4c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n",
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0]])\n",
            "[[0 0 0]\n",
            " [0 0 0]]\n",
            "\n",
            "\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "[[1. 1.]\n",
            " [1. 1.]]\n",
            "\n",
            "\n",
            "tensor([3, 5, 7, 9])\n",
            "[3 5 7 9]\n",
            "\n",
            "\n",
            "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
            "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor 기본 연산 (Element-wise vs Dot Product)\n",
        "\n",
        "PyTorch 연산에서 가장 중요한 것은 **`*` (단순 곱)**과 **`@` (행렬 곱)**을 구분하는 것입니다.\n",
        "\n",
        "### 1. 원소별 연산 (Element-wise)\n",
        "기본 산술 연산자(`+`, `-`, `*`, `/`, `**`)는 **같은 위치(Index)에 있는 원소끼리** 1:1로 계산합니다. 결과의 모양(Shape)이 유지됩니다.\n",
        "\n",
        "```python\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([3, 2, 1])\n",
        "\n",
        "# [1*3, 2*2, 3*1]\n",
        "print(a * b)   # tensor([3, 4, 3]) -> 이를 아다마르 곱(Hadamard Product)이라고도 함\n",
        "\n",
        "# [1^2, 2^2, 3^2]\n",
        "print(a ** 2)  # tensor([1, 4, 9])\n",
        "```\n",
        "\n",
        "### 2. 내적 / 행렬 곱 (Dot Product)\n",
        "**`@` 연산자**는 벡터의 내적(Inner Product) 또는 행렬 곱셈을 수행합니다. 1차원 벡터끼리 연산하면 결과가 하나의 값(Scalar)으로 합쳐집니다.\n",
        "\n",
        "```python\n",
        "# 계산 과정: (1*3) + (2*2) + (3*1) = 3 + 4 + 3 = 10\n",
        "print(a @ b)   # tensor(10)\n",
        "```\n",
        "\n",
        "> **Tip:** 과거에는 `torch.matmul(a, b)`를 많이 썼지만, 최신 코드에서는 가독성을 위해 **`a @ b`**를 더 선호합니다."
      ],
      "metadata": {
        "id": "CegvZSWpYwHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2,3])\n",
        "b = torch.tensor([3,2,1])\n",
        "print(a+b)\n",
        "print(a*b) #\n",
        "print(a@b) #\n",
        "\n",
        "print(a/b)\n",
        "print(a**2)  # 제곱도 각 성분에 대해서."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XaR94r-YANI",
        "outputId": "d5c6147b-ac7c-4835-af6c-18a765f0267d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4, 4, 4])\n",
            "tensor([3, 4, 3])\n",
            "tensor(10)\n",
            "tensor([0.3333, 1.0000, 3.0000])\n",
            "tensor([1, 4, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pytorch의 인덱싱과 슬라이싱\n",
        "--> numpy, list와 똑같음."
      ],
      "metadata": {
        "id": "1_ng9DjMZ3vG"
      }
    }
  ]
}