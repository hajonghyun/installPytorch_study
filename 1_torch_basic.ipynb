{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxcVSDnN4wKUgIwucUiGd4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hajonghyun/installPytorch_study/blob/main/1_torch_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2xdvTxOWPE3S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.tensor` vs `np.array` 핵심 비교\n",
        "\n",
        "AI 엔지니어링, 특히 딥러닝 학습 단계로 넘어갈 때 가장 명확하게 이해해야 하는 두 자료형의 차이점입니다.\n",
        "\n",
        "### 1. 요약 표\n",
        "\n",
        "| 비교 항목 | NumPy (`np.array`) | PyTorch (`torch.tensor`) |\n",
        "| :--- | :--- | :--- |\n",
        "| **주 용도** | 일반 수치 계산, 데이터 전처리, 머신러닝(Scikit-learn) | **딥러닝 모델 학습**, GPU 가속 연산 |\n",
        "| **하드웨어** | **CPU** 연산만 가능 | **CPU + GPU** 연산 지원 (`.to('cuda')`) |\n",
        "| **미분(Gradient)** | 지원 안 함 (수식 직접 구현 필요) | **Autograd** (자동 미분) 지원 (`requires_grad=True`) |\n",
        "| **메모리 공유** | `from_numpy()` 사용 시 메모리 공유 가능 | `torch.as_tensor()` 등으로 효율적 변환 가능 |\n",
        "\n",
        "---\n",
        "\n",
        "### 2. 주요 차이점 상세\n",
        "\n",
        "#### A. GPU 가속 (Hardware Acceleration)\n",
        "**가장 큰 차이점**입니다. NumPy는 CPU에서만 작동하지만, Tensor는 GPU로 데이터를 옮겨 대규모 병렬 연산(행렬 곱 등)을 빠르게 처리할 수 있습니다.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# NumPy: 오직 CPU 메모리 사용\n",
        "arr = np.array([1, 2, 3])\n",
        "\n",
        "# PyTorch: GPU로 이동 가능\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "if torch.cuda.is_available():\n",
        "    tensor = tensor.to('cuda') # 데이터를 GPU VRAM에 올림\n",
        "```\n",
        "\n",
        "#### B. 자동 미분 (Autograd)\n",
        "딥러닝의 핵심인 **역전파(Backpropagation)**를 수행하기 위해, PyTorch Tensor는 연산의 히스토리를 추적하고 미분값을 저장할 수 있습니다.\n",
        "\n",
        "```python\n",
        "# requires_grad=True: 이 텐서에 대한 연산을 추적하겠다는 의미\n",
        "w = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "y = w ** 2      # 수식: y = w^2\n",
        "y.backward()    # 미분 수행 (dy/dw)\n",
        "\n",
        "print(w.grad)   # 결과: 4.0 (2 * w)\n",
        "```\n",
        "* **NumPy**는 단순히 값을 저장하고 계산할 뿐, 미분 계수(`grad`)를 자동으로 계산해주지 않습니다.\n",
        "\n",
        "### 3. 결론 (AI 엔지니어 관점)\n",
        "* **데이터 전처리/분석 단계:** `np.array`와 Pandas를 주로 사용합니다.\n",
        "* **모델 학습/추론 단계:** 데이터를 `torch.tensor`로 변환하여 GPU에 올리고 모델에 주입합니다."
      ],
      "metadata": {
        "id": "OqE9dGFdSzCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([1,2,3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJw-CN7_PdhT",
        "outputId": "638efbfa-d27b-4a4d-8730-e67b79e82b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([1,2,3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih9waJeiPkDs",
        "outputId": "37035b35-4c6f-49e4-e3b8-22ee5a718e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `type()` vs `.dtype` 차이점 정리\n",
        "\n",
        "파이썬으로 데이터 사이언스나 딥러닝을 할 때, **디버깅(에러 해결)의 80%는 이 두 가지를 구분하는 것**에서 시작됩니다.\n",
        "\n",
        "### 1. 핵심 요약\n",
        "\n",
        "| 구분 | 명령어 예시 | 설명 | 비유 (컵과 내용물) |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **`type()`** | `type(data)` | **객체(컨테이너) 자체의 자료형**을 확인합니다.<br>(예: 이것은 리스트인가? 텐서인가?) | **\"이 컵은 유리컵인가, 머그컵인가?\"** |\n",
        "| **`.dtype`** | `data.dtype` | **객체 안에 담긴 데이터(원소)의 자료형**을 확인합니다.<br>(예: 텐서 안에 들어있는 숫자가 `int`인가 `float`인가?) | **\"컵 안에 든 것이 물인가, 콜라인가?\"** |\n",
        "\n",
        "---\n",
        "\n",
        "### 2. 코드 예시 (PyTorch & NumPy)\n",
        "\n",
        "가장 흔히 혼동하는 상황을 코드로 확인해 보세요.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 데이터 생성\n",
        "np_arr = np.array([1.0, 2.0, 3.0])\n",
        "torch_tensor = torch.tensor([1, 2, 3]) # 정수로 생성\n",
        "\n",
        "# 1. type(): 껍데기 확인\n",
        "print(type(np_arr))       # <class 'numpy.ndarray'> -> \"이것은 넘파이 배열입니다.\"\n",
        "print(type(torch_tensor)) # <class 'torch.Tensor'>  -> \"이것은 파이토치 텐서입니다.\"\n",
        "\n",
        "# 2. .dtype: 내용물 확인 (매우 중요!)\n",
        "print(np_arr.dtype)       # float64 -> \"안에 실수가 들어있습니다.\"\n",
        "print(torch_tensor.dtype) # torch.int64 -> \"안에 정수가 들어있습니다.\"\n",
        "```\n",
        "\n",
        "### 3. 왜 중요한가요? (AI 엔지니어 관점)\n",
        "\n",
        "* **`type()` 에러:** 주로 **호환성** 문제입니다.\n",
        "    * *예: PyTorch 모델에 실수로 NumPy 배열을 넣으면 `TypeError`가 발생합니다.*\n",
        "* **`.dtype` 에러:** 주로 **정밀도나 연산** 문제입니다.\n",
        "    * *예: 딥러닝 모델 가중치는 보통 `float32`인데, 입력 데이터가 `int64`나 `float64`면 `RuntimeError: expected scalar type Float but found Double` 같은 에러가 뜹니다.*\n",
        "    * **해결:** `tensor.to(torch.float32)` 처럼 캐스팅(형변환)을 해줘야 합니다."
      ],
      "metadata": {
        "id": "OioxCeBwTwrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2,3])\n",
        "b = torch.tensor([1.0, 2, 3]) # 하나라도 실수면 dtype은 float"
      ],
      "metadata": {
        "id": "J7Hv3GNORlPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(a), type(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUnd5I7mPnST",
        "outputId": "50c5622f-1c37-446c-a17d-9dbe5faeed40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.dtype, b.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gngi16aNPs4z",
        "outputId": "2d9d43d4-caf8-4c2d-dd35-42dd36c5ff7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64 torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9dhoVsHRz-u",
        "outputId": "4773c6ff-e9de-41d5-a898-7eb9e01d57cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor 핵심 속성 및 형태 규칙\n",
        "\n",
        "### 1. 형태 규칙 (Shape Constraint)\n",
        "* **직사각형 형태 강제 (Strict Rectangular):** Tensor는 GPU 병렬 연산을 위해 모든 행(Row)의 길이가 반드시 같아야 합니다.\n",
        "    * `[[1, 2, 3], [4, 5]]` ❌ : **Jagged Tensor 불가** (에러 발생)\n",
        "    * `[[1, 2, 3], [4, 5, 6]]` ⭕ : 생성 가능\n",
        "\n",
        "### 2. 필수 속성 조회 3대장\n",
        "\n",
        "딥러닝 모델 디버깅 시 `print()`로 가장 많이 찍어보는 3가지입니다.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "a = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "\n",
        "# 1. 모양 (Shape) - ★가장 중요★\n",
        "# 각 차원의 크기를 확인 (행, 열)\n",
        "print(a.shape)    # torch.Size([2, 3])\n",
        "\n",
        "# 2. 차원 수 (Dimension/Rank)\n",
        "# 몇 차원 텐서인지 확인\n",
        "print(a.ndim)     # 2\n",
        "\n",
        "# 3. 전체 원소 개수 (Number of Elements)\n",
        "# 2행 * 3열 = 총 6개\n",
        "print(a.numel())  # 6\n",
        "```"
      ],
      "metadata": {
        "id": "tnVZp4OAUIYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1,2,3],\n",
        "                  [4,5,6]])\n",
        "# b = torch.tensor([[1,2,3],\n",
        "#                   [4,5]])\n",
        "# np.array와는 달리 torch.tensor는 행렬\n",
        "# 각 행에 해당하는 숫자의 개수 같아야함.\n",
        "\n",
        "print(a.shape)\n",
        "print(a.ndim) # 차원 수\n",
        "print(a.numel()) # num of element 요소의 수"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGWGyPCRT5m2",
        "outputId": "1c9c568f-5981-4d78-a50f-f3ef97586a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n",
            "2\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor 생성 및 초기화 (NumPy와 비교)\n",
        "\n",
        "PyTorch는 NumPy와 매우 유사한 API를 제공하지만, **Shape 입력 방식**에서 더 유연합니다.\n",
        "\n",
        "### 1. 0 또는 1로 채우기 (`zeros`, `ones`)\n",
        "* **핵심 차이:** `torch`는 차원(Shape)을 튜플 `()`로 묶지 않고 **인자로 바로 나열**해도 됩니다. (NumPy는 튜플 필수)\n",
        "\n",
        "```python\n",
        "# 5행 5열 0행렬\n",
        "print(torch.zeros(5, 5))  # OK: 괄호 하나로 충분\n",
        "# print(np.zeros(5, 5))   # Error: np.zeros((5, 5))여야 함\n",
        "\n",
        "# 2행 2열 1행렬\n",
        "print(torch.ones(2, 2))\n",
        "```\n",
        "\n",
        "### 2. 형태 복제하기 (`_like`)\n",
        "* 기존 텐서 `a`의 **Shape(모양), Dtype(자료형), Device(CPU/GPU)** 속성을 그대로 물려받아 새로운 텐서를 만듭니다.\n",
        "\n",
        "```python\n",
        "# a와 똑같은 모양으로 0을 채움\n",
        "print(torch.zeros_like(a))\n",
        "```\n",
        "\n",
        "### 3. 수열 생성 (`arange`, `linspace`)\n",
        "* 데이터 인덱싱이나 그래프 축(x-axis)을 만들 때 주로 사용합니다.\n",
        "\n",
        "| 함수 | 설명 | 인자 의미 | 예시 |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **`arange`** | 간격(Step) 기준 생성 | `(시작, 끝, 간격)` | `torch.arange(0, 10, 2)`<br>→ `[0, 2, 4, 6, 8]` (끝 미포함) |\n",
        "| **`linspace`** | 개수(Count) 기준 생성 | `(시작, 끝, 개수)` | `torch.linspace(0, 10, 5)`<br>→ `[0, 2.5, 5, 7.5, 10]` (끝 포함) |"
      ],
      "metadata": {
        "id": "wGdR5YwuXLXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.zeros(5,5))\n",
        "print(np.zeros((5,5)))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(torch.zeros_like(a))\n",
        "print(np.zeros_like(a))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(torch.ones(2,2))\n",
        "print(np.ones((2,2)))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(torch.arange(3,10,2))\n",
        "print(np.arange(3,10,2))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(torch.linspace(1,10,10))\n",
        "print(np.linspace(1,10,10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zxxNGlSUR9N",
        "outputId": "19b033ca-154a-4967-e44a-efe532fdd4c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n",
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0]])\n",
            "[[0 0 0]\n",
            " [0 0 0]]\n",
            "\n",
            "\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "[[1. 1.]\n",
            " [1. 1.]]\n",
            "\n",
            "\n",
            "tensor([3, 5, 7, 9])\n",
            "[3 5 7 9]\n",
            "\n",
            "\n",
            "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
            "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor 기본 연산 (Element-wise vs Dot Product)\n",
        "\n",
        "PyTorch 연산에서 가장 중요한 것은 **`*` (단순 곱)**과 **`@` (행렬 곱)**을 구분하는 것입니다.\n",
        "\n",
        "### 1. 원소별 연산 (Element-wise)\n",
        "기본 산술 연산자(`+`, `-`, `*`, `/`, `**`)는 **같은 위치(Index)에 있는 원소끼리** 1:1로 계산합니다. 결과의 모양(Shape)이 유지됩니다.\n",
        "\n",
        "```python\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([3, 2, 1])\n",
        "\n",
        "# [1*3, 2*2, 3*1]\n",
        "print(a * b)   # tensor([3, 4, 3]) -> 이를 아다마르 곱(Hadamard Product)이라고도 함\n",
        "\n",
        "# [1^2, 2^2, 3^2]\n",
        "print(a ** 2)  # tensor([1, 4, 9])\n",
        "```\n",
        "\n",
        "### 2. 내적 / 행렬 곱 (Dot Product)\n",
        "**`@` 연산자**는 벡터의 내적(Inner Product) 또는 행렬 곱셈을 수행합니다. 1차원 벡터끼리 연산하면 결과가 하나의 값(Scalar)으로 합쳐집니다.\n",
        "\n",
        "```python\n",
        "# 계산 과정: (1*3) + (2*2) + (3*1) = 3 + 4 + 3 = 10\n",
        "print(a @ b)   # tensor(10)\n",
        "```\n",
        "\n",
        "> **Tip:** 과거에는 `torch.matmul(a, b)`를 많이 썼지만, 최신 코드에서는 가독성을 위해 **`a @ b`**를 더 선호합니다."
      ],
      "metadata": {
        "id": "CegvZSWpYwHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2,3])\n",
        "b = torch.tensor([3,2,1])\n",
        "print(a+b)\n",
        "print(a*b) #\n",
        "print(a@b) #\n",
        "\n",
        "print(a/b)\n",
        "print(a**2)  # 제곱도 각 성분에 대해서."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XaR94r-YANI",
        "outputId": "d5c6147b-ac7c-4835-af6c-18a765f0267d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4, 4, 4])\n",
            "tensor([3, 4, 3])\n",
            "tensor(10)\n",
            "tensor([0.3333, 1.0000, 3.0000])\n",
            "tensor([1, 4, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pytorch의 인덱싱과 슬라이싱\n",
        "--> numpy, list와 똑같음."
      ],
      "metadata": {
        "id": "1_ng9DjMZ3vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2,3,4,5,6,7,8,9])\n",
        "\n",
        "print(a[7:])\n",
        "print(a[2:5])\n",
        "print(a[::2])\n",
        "print(a[:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0p3bJVnumGU",
        "outputId": "d4da7d40-24e4-4ce1-e8e4-300a102e6d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([8, 9])\n",
            "tensor([3, 4, 5])\n",
            "tensor([1, 3, 5, 7, 9])\n",
            "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1,2,3],\n",
        "                  [4,5,6],\n",
        "                  [7,8,9]])\n",
        "print(a[0])\n",
        "print(a[-1])\n",
        "print(a[1:])\n",
        "print(a[:])\n",
        "print(a[0][2])\n",
        "print(a[0,2])   # 일반 list에서는 불가! np, torch에서만 가능\n",
        "\n",
        "print(a[:][2]) # a[:] == a\n",
        "print(a[:,2]) # 위와 결과가 다름을 알 수 있음.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gR_2Bd-0ut_0",
        "outputId": "2bcc20cb-cbf8-4609-83cb-b072bf3ccf3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "tensor([7, 8, 9])\n",
            "tensor([[4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "tensor(3)\n",
            "tensor(3)\n",
            "tensor([7, 8, 9])\n",
            "tensor([3, 6, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3차원 텐서와 차원(Shape) 규칙\n",
        "\n",
        "### 1. 3차원 텐서 구조 해석 (`2, 3, 4`)\n",
        "텐서의 Shape은 **가장 바깥쪽 대괄호부터 안쪽으로** 파고들며 해석합니다.\n",
        "\n",
        "* `torch.Size([2, 3, 4])`\n",
        "    * **2 (Depth):** 큰 덩어리(면)가 2개\n",
        "    * **3 (Row):** 각 덩어리 안에 행이 3개\n",
        "    * **4 (Col):** 각 행 안에 열(원소)이 4개\n",
        "\n",
        "### 2. 대괄호와 차원의 관계 (The Bracket Rule)\n",
        "**\"대괄호 `[]`를 한 겹 씌울 때마다, Shape의 맨 왼쪽에 `1`이 추가됩니다.\"**\n",
        "\n",
        "내용물(데이터)은 같아도, 대괄호로 감싸면 **차원(Rank)**이 높아집니다.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# 1. 기본: 1차원 벡터 (요소 4개)\n",
        "a = torch.tensor([1, 2, 3, 4])\n",
        "print(a.shape) # torch.Size([4])\n",
        "\n",
        "# 2. 대괄호 1겹 추가 -> 2차원 (1행 4열)\n",
        "b = torch.tensor([[1, 2, 3, 4]])\n",
        "print(b.shape) # torch.Size([1, 4])\n",
        "\n",
        "# 3. 대괄호 2겹 추가 -> 3차원 (덩어리 1개, 1행 4열)\n",
        "c = torch.tensor([[[1, 2, 3, 4]]])\n",
        "print(c.shape) # torch.Size([1, 1, 4])\n",
        "\n",
        "# 4. 대괄호 3겹 추가 -> 4차원\n",
        "d = torch.tensor([[[[1, 2, 3, 4]]]])\n",
        "print(d.shape) # torch.Size([1, 1, 1, 4])\n",
        "```\n",
        "\n",
        "> **Tip:** 이렇게 `1`인 차원이 불필요하게 생겼을 때는 **`squeeze()`** 함수로 제거할 수 있습니다."
      ],
      "metadata": {
        "id": "cqE-ObBlxJ1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 인덱싱의 기본\n",
        "\n",
        "### **변수 오른쪽에 []가 열리면** 뭘 하겠다는 뜻?\n",
        "\n",
        "### ⏩ 인덱싱을 하겠다."
      ],
      "metadata": {
        "id": "Xr0uAYSORcbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3차원 행렬 인덱싱\n",
        "a = torch.tensor([[[0,1,2,3],[4,5,6,7],[8,9,10,11]],\n",
        "                  [[12,13,14,15],[16,17,18,19],[20,21,22,23]]])\n",
        "print(a)\n",
        "print(a.shape)\n",
        "print(a.ndim)\n",
        "\n",
        "\n",
        "# 대괄호가 하나 늘어나면 왼쪽에 shape값이 추가된다.\n",
        "a = torch.tensor([[1,2,3,4]])\n",
        "print(a.shape)\n",
        "\n",
        "a = torch.tensor([[[1,2,3,4]]])\n",
        "print(a.shape)\n",
        "\n",
        "a = torch.tensor([[[[1,2,3,4]]]])\n",
        "print(a.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io3XLmSRvo1U",
        "outputId": "0fdf61be-0801-4306-92e9-36789c85bfc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0,  1,  2,  3],\n",
            "         [ 4,  5,  6,  7],\n",
            "         [ 8,  9, 10, 11]],\n",
            "\n",
            "        [[12, 13, 14, 15],\n",
            "         [16, 17, 18, 19],\n",
            "         [20, 21, 22, 23]]])\n",
            "torch.Size([2, 3, 4])\n",
            "3\n",
            "torch.Size([1, 4])\n",
            "torch.Size([1, 1, 4])\n",
            "torch.Size([1, 1, 1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 여러번 보기\n",
        "\n",
        "https://www.youtube.com/watch?v=Jpb7Z3w6ZmI&list=LL&index=3&t=1206s"
      ],
      "metadata": {
        "id": "1gPAY86m_e8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch 팬시 인덱싱 (Fancy Indexing)\n",
        "\n",
        "슬라이싱(`:`)이 범위를 칼로 자르는 것이라면, 팬시 인덱싱은 **원하는 지점만 콕콕 집어내거나(Picking)** 순서를 내 마음대로 섞는 기법입니다.\n",
        "\n",
        "## 1. 판단 기준: \"이게 팬시 인덱싱인가?\"\n",
        "대괄호 `[]` 안에 **숫자 하나**나 **슬라이싱(`:`)**이 아닌, **또 다른 리스트(List)나 배열(Tensor)**이 들어가 있다면 팬시 인덱싱입니다.\n",
        "\n",
        "* `a[1:3]` : 슬라이싱 (범위)\n",
        "* `a[[1, 3]]` : **팬시 인덱싱** (1번과 3번만 선택)\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 핵심 해석법: \"메인 콤마(,)의 법칙\" ★★★\n",
        "\n",
        "복잡한 대괄호 속에서 길을 잃지 않는 유일한 법칙입니다. **가장 바깥쪽 콤마**가 차원을 가르는 국경선입니다.\n",
        "\n",
        "### Case A. 콤마가 있다? -> `a[ [0,1], [0,1] ]`\n",
        "* **의미:** 차원별로 좌표를 따로 줬다는 뜻입니다.\n",
        "* **해석법:** **\"좌표 찍기 (Zipping)\"**\n",
        "    * 리스트들을 세로로 나란히 놓고 같은 순서끼리 묶어서 $(x, y)$ 좌표를 만듭니다.\n",
        "    * 예: `(0,0)` 좌표의 값 하나, `(1,1)` 좌표의 값 하나.\n",
        "\n",
        "### Case B. 콤마가 없다? -> `a[ [0,1] ]`\n",
        "* **의미:** 리스트 전체가 **첫 번째 차원(Dim 0)**에 통째로 들어갔다는 뜻입니다.\n",
        "* **해석법:** **\"덩어리 선택 (Selection)\"**\n",
        "    * \"0번 덩어리와 1번 덩어리를 통째로 가져와라.\"\n",
        "    * 남은 차원은 건드리지 않았으므로 그대로 유지됩니다."
      ],
      "metadata": {
        "id": "w8gS9l8OyK41"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8A8T4XMB6SKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDS8LX3v6SIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[[0,1,2,3],[4,5,6,7],[8,9,10,11]],\n",
        "                  [[12,13,14,15],[16,17,18,19],[20,21,22,23]]])\n",
        "print(a)\n",
        "a[[0,1,1,0],[0,1,2,1],[3,3,2,1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmORxLfAyLCc",
        "outputId": "7f764708-c4f9-4f61-adc6-9a182c2fc44f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0,  1,  2,  3],\n",
            "         [ 4,  5,  6,  7],\n",
            "         [ 8,  9, 10, 11]],\n",
            "\n",
            "        [[12, 13, 14, 15],\n",
            "         [16, 17, 18, 19],\n",
            "         [20, 21, 22, 23]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3, 19, 22,  5])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 팬시 인덱싱 점검: 4차원 텐서 퀴즈\n",
        "\n",
        "제대로 이해했는지 확인하는 고난도 4차원 문제입니다.\n",
        "\n",
        "### 기본 설정\n",
        "0부터 15까지 숫자가 채워진 `(2, 2, 2, 2)` 텐서입니다.\n",
        "```python\n",
        "import torch\n",
        "a = torch.arange(16).view(2, 2, 2, 2)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "o4HL1AkR6lvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shape: (2, 2, 2, 2) -> (B, C, H, W)라고 상상해보세요.\n",
        "a = torch.arange(16).view(2, 2, 2, 2)\n",
        "\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0gEtZlM3QqK",
        "outputId": "6b1735f8-d8db-4f4d-de9c-2542e2389f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0,  1],\n",
            "          [ 2,  3]],\n",
            "\n",
            "         [[ 4,  5],\n",
            "          [ 6,  7]]],\n",
            "\n",
            "\n",
            "        [[[ 8,  9],\n",
            "          [10, 11]],\n",
            "\n",
            "         [[12, 13],\n",
            "          [14, 15]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. 좌표 콕콕 찍기 (Basic)\n",
        "리스트가 모든 차원에 다 들어간 경우입니다.\n",
        "```python\n",
        "# 힌트: 세로로 묶어서 좌표(0,0,0,0)과 (1,1,1,1)을 만드세요.\n",
        "print(a[[0, 1], [0, 1], [0, 1], [0, 1]])\n",
        "```\n",
        "> **정답:** `tensor([0, 15])`\n",
        "> **해설:** 점 2개를 핀셋으로 집어낸 결과이므로 1차원 벡터가 됩니다."
      ],
      "metadata": {
        "id": "qQc25lAF6upp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a[[0, 1], [0, 1], [0, 1], [0, 1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9E4O1mB3Rc0",
        "outputId": "91348668-8b21-46b7-8b1d-8de35be94b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. 고정과 선택의 조화 (Intermediate)\n",
        "앞쪽 차원은 숫자로 고정하고, 뒤쪽 차원만 리스트로 선택하는 경우입니다.\n",
        "```python\n",
        "# 힌트: 앞의 0, 1은 고정! 뒤의 [0,1], [1,0]만 짝을 지으세요.\n",
        "print(a[0, 1, [0, 1], [1, 0]])\n",
        "```\n",
        "> **정답:** `tensor([5, 6])`\n",
        "> **해설:**\n",
        "> * 좌표 1: `(0, 1, 0, 1)` -> 값 5\n",
        "> * 좌표 2: `(0, 1, 1, 0)` -> 값 6"
      ],
      "metadata": {
        "id": "p-Qw3lZh60uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a[0, 1, [0, 1], [1, 0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTb4Xg5v4FPF",
        "outputId": "318e2c40-58d0-4c16-cfe8-792c0d3f06b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. 덩어리째 가져오기 (Advanced)\n",
        "**메인 콤마가 없는** 경우입니다.\n",
        "```python\n",
        "# 힌트: [1, 1, 0] 리스트 전체가 Dim 0에 적용됩니다.\n",
        "print(a[[1, 1, 0]].shape)\n",
        "```\n",
        "> **정답:** `torch.Size([3, 2, 2, 2])`\n",
        "> **해설:**\n",
        "> * Dim 0에서 1번, 1번, 0번 덩어리를 순서대로 가져왔으므로 개수가 **3개**가 됩니다.\n",
        "> * 뒤쪽 차원 `(2, 2, 2)`는 건드리지 않았으므로 그대로 유지됩니다."
      ],
      "metadata": {
        "id": "vQy3iwVw63OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a[[1, 1, 0]].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gma6U8ZA40xJ",
        "outputId": "139ff5dd-3f66-48b1-e8c4-d3a16f9c9136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Boolean Indexing\n",
        "## 불리언 인덱싱 (Boolean Indexing / Masking)\n",
        "\n",
        "인덱스(숫자)가 아닌 **조건(True/False)**을 이용해 데이터를 필터링하거나 값을 변경하는 기법입니다.\n",
        "\n",
        "### 1. 핵심 원리\n",
        "* **규칙:** `True`인 위치의 데이터는 **선택(Keep)**하고, `False`인 위치는 **버립니다(Drop)**.\n",
        "* **제약:** 마스크(True/False 리스트)의 길이는 데이터의 해당 차원 길이와 **반드시 같아야 합니다.**\n",
        "\n",
        "### 2. 코드 예시\n",
        "\n",
        "#### A. 텐서(Tensor)에서의 동작 (가능)\n",
        "행(Row) 단위로 필터링하는 경우입니다.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "a = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]]) # 4행 2열\n",
        "mask = torch.tensor([True, False, False, True])    # 길이 4\n",
        "\n",
        "# mask가 True인 0번행, 3번행만 선택\n",
        "print(a[mask, :])\n",
        "# 결과: tensor([[1, 2], [7, 8]])\n",
        "```\n",
        "\n",
        "#### B. 파이썬 리스트(List)에서의 동작 (불가능)\n",
        "리스트는 불리언 인덱싱을 지원하지 않습니다.\n",
        "\n",
        "```python\n",
        "c = [1, 2, 3, 4]\n",
        "# c[[True, True, False, False]]\n",
        "# -> TypeError: list indices must be integers or slices (에러 발생)\n",
        "```\n",
        "\n",
        "### 3. 실무 활용 (조건부 값 변경)\n",
        "AI 전처리에서 가장 많이 쓰는 패턴입니다. **\"특정 조건에 맞는 값만 골라서 다른 값으로 덮어쓰기\"**가 가능합니다.\n",
        "\n",
        "```python\n",
        "data = torch.tensor([1, -1, 2, -5, 3])\n",
        "\n",
        "# \"0보다 작은 값(음수)은 모두 0으로 바꿔라\" (ReLU 처럼)\n",
        "data[data < 0] = 0\n",
        "\n",
        "print(data)\n",
        "# 결과: tensor([1, 0, 2, 0, 3])\n",
        "```"
      ],
      "metadata": {
        "id": "Ga1nsXyoPB2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# boolean 인덱싱\n",
        "a = [1,2,3,4,5,3,3]\n",
        "print(a==3)  # 여러개가 들어있는 리스트랑 달랑 3하나랑 같냐? => 당연히 다름\n",
        "\n",
        "a = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
        "print(a==3)  # 리스트와 다르게 '각 성분'에 대해 비교를 해줌!!\n",
        "\n",
        "print(a>3)\n",
        "\n",
        "print(a[a>3]) # 불리안 인덱싱! (True,False가 담긴 행렬로 인덱싱)\n",
        "\n",
        "# 이런 것도 가능: 특정 조건을 만족하는 애들을 뭘로 바꿔라.\n",
        "# masked self attention에서 사용.\n",
        "a[a>3] = 1000\n",
        "print(a)\n",
        "\n",
        "\n",
        "# 위와 같은 결과. boolean indexing을 사용하는 이유.\n",
        "for i in range(a.shape[0]):\n",
        "    for j in range(a.shape[1]):\n",
        "        if a[i,j] > 3:\n",
        "            a[i,j]=1000\n",
        "\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnWk9EeXPB_-",
        "outputId": "0cadacf4-6671-45f6-a9a3-9770a34277af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "tensor([[False, False,  True, False],\n",
            "        [False, False, False, False]])\n",
            "tensor([[False, False, False,  True],\n",
            "        [ True,  True,  True,  True]])\n",
            "tensor([4, 5, 6, 7, 8])\n",
            "tensor([[   1,    2,    3, 1000],\n",
            "        [1000, 1000, 1000, 1000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a= torch.tensor([[1,2],[3,4],[5,6],[7,8]])\n",
        "b= torch.tensor([True,False,False,True])\n",
        "print(a[b,:]) # 행에만 boolean indexing\n",
        "\n",
        "b = torch.tensor([1,2,3,4])\n",
        "print(b[[True,True,False,False]]) # 가능\n",
        "\n",
        "c = [1,2,3,4]\n",
        "# print(c[[True,True,False,False]]) # 불가능\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO-ha96JPy3S",
        "outputId": "871a1aed-5fe4-4f36-a5ce-a4c045558baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [7, 8]])\n",
            "tensor([1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Tesor 인덱싱"
      ],
      "metadata": {
        "id": "-n4hI8eQvIYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a= torch.tensor([[1,2,3],[4,5,6]])\n",
        "# 숫자에 교체투입\n",
        "\n",
        "# 숫자에 해당하는 a[0]을 대괄호 포함해서 가져온다.\n",
        "print(a[torch.tensor(0)])\n",
        "\n",
        "# 숫자에 해당하는 a[0]을 대괄호 포함해서 가져오는데, 대괄호가 밖에 쌓여있음.\n",
        "print(a[torch.tensor([0])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozpTzxnyWj88",
        "outputId": "4a766939-114e-4106-9cde-ed97e7819a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "tensor([[1, 2, 3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2,3,4,5])\n",
        "\n",
        "# step1: 일반 인덱싱\n",
        "A = a[2]\n",
        "print(a)\n",
        "\n",
        "# step2: torch.tensor 가 들어있는 인덱싱\n",
        "A = a[torch.tensor([2,3,4])]\n",
        "print(A)\n",
        "\n",
        "# step3: 인덱싱된 애들로 2행 3열짜리 행렬을 만든다.\n",
        "A = a[torch.tensor([[2,2,2],[3,3,3]])]\n",
        "print(A)\n",
        "\n",
        "# cf. 일반 리스트\n",
        "a = [1,2,3]\n",
        "# a[[1,1,1,2,2,2]] # error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWp7IyOFR5W-",
        "outputId": "cc139de6-1acb-4946-816e-7ed3fbdec8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3, 4, 5])\n",
            "tensor([3, 4, 5])\n",
            "tensor([[3, 3, 3],\n",
            "        [4, 4, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1,2,3],[4,5,6]])\n",
        "\n",
        "# step1: 일반 인덱싱\n",
        "A = a[0]\n",
        "print(A)\n",
        "\n",
        "# step2: torch.tensor 가 들어있는 인덱싱\n",
        "A = a[torch.tensor([0,1])]\n",
        "print(A)\n",
        "print(A.shape)\n",
        "\n",
        "# step3: 인덱싱된 애들로 (2*2)*3 행렬을 만든다.\n",
        "A = a[torch.tensor([[0,0],[1,1]])]\n",
        "print(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqCfv-twuMgZ",
        "outputId": "695c8e87-2da2-4819-e5dc-bb926b44fa2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "torch.Size([2, 3])\n",
            "tensor([[[1, 2, 3],\n",
            "         [1, 2, 3]],\n",
            "\n",
            "        [[4, 5, 6],\n",
            "         [4, 5, 6]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a= torch.tensor([[1,2,3],[4,5,6]])\n",
        "A= a[torch.tensor([[0,1],[1,1]])]\n",
        "print(A.shape) #a[0]는 1차원 데이터이므로 한 차원이 뒤에 늘어나서 2,2,\"3\" 이 된다.\n",
        "print(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aakSTIbVaCmJ",
        "outputId": "ad726449-07c7-4b7b-c3a1-b41917d07e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 3])\n",
            "tensor([[[1, 2, 3],\n",
            "         [4, 5, 6]],\n",
            "\n",
            "        [[4, 5, 6],\n",
            "         [4, 5, 6]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b= torch.tensor([[225,255,0],[0,255,0],[0,0,255],[255,0,255],[70,80,75],[0,0,4],[60,100,255]])\n",
        "B = b[torch.tensor([[0,1],[2,3]])]\n",
        "print(B)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "WY8zUf3EdLIZ",
        "outputId": "5a5ab2d0-9ee2-4587-eaed-f8a561a061bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[225, 255,   0],\n",
            "         [  0, 255,   0]],\n",
            "\n",
            "        [[  0,   0, 255],\n",
            "         [255,   0, 255]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7c54fa902bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 인덱싱 연습문제"
      ],
      "metadata": {
        "id": "BvYVXk6KvgiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1,2,3],[4,5,6]])\n",
        "\n",
        "A = a[torch.tensor([[0,1],[1,1]])]\n",
        "print(A)\n",
        "\n",
        "# A와 동일한 결과를 리스트 인덱싱으로 만들어보자.\n",
        "\n",
        "B = a[\n",
        "    [[[0,0,0],[1,1,1]],[[1,1,1],[1,1,1]]],  #행\n",
        "     [[[0,1,2],[0,1,2]],[[0,1,2],[0,1,2]]]  #열\n",
        "    ]\n",
        "print(B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgTl6L8DviyR",
        "outputId": "fa839606-a748-45c2-9b0e-d65dbcd3e0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1, 2, 3],\n",
            "         [4, 5, 6]],\n",
            "\n",
            "        [[4, 5, 6],\n",
            "         [4, 5, 6]]])\n",
            "tensor([[[1, 2, 3],\n",
            "         [4, 5, 6]],\n",
            "\n",
            "        [[4, 5, 6],\n",
            "         [4, 5, 6]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 최종 비교 정리"
      ],
      "metadata": {
        "id": "cfFdgqJh3PMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.tensor([[1,2,6],[3,4,7],[5,6,2],[7,8,9]])\n",
        "print(A)\n",
        "print(A.shape)\n",
        "\n",
        "# 1. A[몇 행, 몇 열]\n",
        "print(A[0,1])\n",
        "\n",
        "#리스트 인덱싱 = 행 리스트와 열 리스트가 합쳐져서 하나가 된다!!!!!! => 무슨 shape을 따라가는지 주목하자.\n",
        "\n",
        "# 2. A[[몇 행, 몇 행],[몇 열,몇 열]]\n",
        "print(A[[0,2,3,1,2],[1,1,0,0,0]])\n",
        "\n",
        "# 2-1. A[ [[몇 행, 몇 행],[몇 행, 몇 행]] , [[몇 열,몇 열],[몇 열, 몇 열]] ]\n",
        "print(A[ [[0,2],[3,1]],[[0,2],[1,0]] ])\n",
        "\n",
        "\n",
        "# 불리안 인덱싱\n",
        "# 3. A[tensor(bool)] => A와 같은 shape을 가지는 tensor형 bool이 어디에 True를 가지고 있나.\n",
        "print(A[torch.tensor([[True,True,True],[True,True,True],[True,True,True],[True,True,True]])])\n",
        "print(A[A==2])\n",
        "\n",
        "# 리스트 인덱싱 + 불리안 인덱싱\n",
        "# 4. A[몇번째 값에 True가 있나, 몇번째 값에 True가 있나]\n",
        "print(A[[True,False,False,False],[False,True,True]])\n",
        "print(A[[0],[1,2]])\n",
        "\n",
        "# tensor 인덱싱 => 몇번째 것을 교체투입할거냐\n",
        "print(A[torch.tensor([1,1,2,2,2])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ388fua3Q6d",
        "outputId": "c912452d-38e3-43f3-b123-85512acf5cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 6],\n",
            "        [3, 4, 7],\n",
            "        [5, 6, 2],\n",
            "        [7, 8, 9]])\n",
            "torch.Size([4, 3])\n",
            "tensor(2)\n",
            "tensor([2, 6, 7, 3, 5])\n",
            "tensor([[1, 2],\n",
            "        [8, 3]])\n",
            "tensor([1, 2, 6, 3, 4, 7, 5, 6, 2, 7, 8, 9])\n",
            "tensor([2, 2])\n",
            "tensor([2, 6])\n",
            "tensor([2, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 파이토치의 여러 함수들\n",
        "\n",
        "## 엄격한 규칙: 파이토치 함수들 안에는 torch.tensor 형태만 넣을 수 있다!!"
      ],
      "metadata": {
        "id": "GL4QkcarZgu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "a=torch.randn(3,3) # normal의 n\n",
        "b=torch.rand(3,3) # 이건 uniform\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuGAZZgNZjBG",
        "outputId": "d0c211be-e0c9-47bd-9f56-adc245672c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.0054,  1.4954,  1.5113],\n",
            "        [-0.2348, -0.3753,  0.7386],\n",
            "        [ 0.1192,  2.0441,  2.1649]])\n",
            "tensor([[0.6018, 0.3889, 0.6865],\n",
            "        [0.4726, 0.0887, 0.1846],\n",
            "        [0.7561, 0.0508, 0.8753]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.randn(3,3)\n",
        "print(a)\n",
        "print(torch.abs(a)) # 절댓값\n",
        "print(torch.sqrt(torch.abs(a))) # 루트\n",
        "print(torch.exp(a))\n",
        "print(torch.log(torch.abs(a))) # 로그\n",
        "print(torch.log(torch.exp(torch.tensor(1))))\n",
        "print(torch.log10(torch.tensor(1000)))\n",
        "print(torch.log2(torch.tensor(8)))\n",
        "print(torch.round(a)) # 반올림\n",
        "print(torch.round(a,decimals=2)) # 소수점 둘째자리까지\n",
        "print(torch.floor(a)) # 내림\n",
        "print(torch.ceil(a)) # 올림"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7MFNsB1bL2j",
        "outputId": "4517ca2a-47ad-48fb-eb68-b1541f8211ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1857,  3.2164,  0.7337],\n",
            "        [-0.8639, -1.2318,  1.4844],\n",
            "        [ 0.0416,  0.9250, -1.7199]])\n",
            "tensor([[0.1857, 3.2164, 0.7337],\n",
            "        [0.8639, 1.2318, 1.4844],\n",
            "        [0.0416, 0.9250, 1.7199]])\n",
            "tensor([[0.4309, 1.7934, 0.8566],\n",
            "        [0.9295, 1.1099, 1.2184],\n",
            "        [0.2039, 0.9618, 1.3115]])\n",
            "tensor([[ 0.8306, 24.9377,  2.0828],\n",
            "        [ 0.4215,  0.2918,  4.4125],\n",
            "        [ 1.0424,  2.5219,  0.1791]])\n",
            "tensor([[-1.6838,  1.1683, -0.3096],\n",
            "        [-0.1463,  0.2085,  0.3950],\n",
            "        [-3.1807, -0.0780,  0.5423]])\n",
            "tensor(1.)\n",
            "tensor(3.)\n",
            "tensor(3.)\n",
            "tensor([[-0.,  3.,  1.],\n",
            "        [-1., -1.,  1.],\n",
            "        [ 0.,  1., -2.]])\n",
            "tensor([[-0.1900,  3.2200,  0.7300],\n",
            "        [-0.8600, -1.2300,  1.4800],\n",
            "        [ 0.0400,  0.9200, -1.7200]])\n",
            "tensor([[-1.,  3.,  0.],\n",
            "        [-1., -2.,  1.],\n",
            "        [ 0.,  0., -2.]])\n",
            "tensor([[-0.,  4.,  1.],\n",
            "        [-0., -1.,  2.],\n",
            "        [ 1.,  1., -1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(torch.pi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujs86zbscQ1E",
        "outputId": "c211b609-e1be-47d6-d19e-8543857743c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "float"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.sin(torch.tensor(torch.pi/6)))\n",
        "print(torch.sin(torch.tensor(torch.pi/2)))\n",
        "print(torch.cos(torch.tensor(torch.pi/3)))\n",
        "print(torch.cos(torch.tensor(torch.pi)))\n",
        "print(torch.tanh(torch.tensor(-10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sOw4Vj1e8-Z",
        "outputId": "2267bf32-d7f8-4a01-927f-e5ddd14412e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5000)\n",
            "tensor(1.)\n",
            "tensor(0.5000)\n",
            "tensor(-1.)\n",
            "tensor(-1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(torch.tensor(1)/6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm2SEIkAgAiD",
        "outputId": "dbd6bd20-a991-4a63-83fd-9ea2c3832467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.isnan # not a number\n",
        "print(torch.log(torch.tensor(-1)))\n",
        "print(torch.isnan(torch.tensor([1,2,torch.nan,3,4])))\n",
        "print(torch.isinf(torch.tensor([1,2,torch.nan,4,torch.inf])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck5SxBT4gSDw",
        "outputId": "f6bc2a49-c7bc-4916-8ba4-30a6c1e1bc1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(nan)\n",
            "tensor([False, False,  True, False, False])\n",
            "tensor([False, False, False, False,  True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"#1\",end='\\n')\n",
        "a = torch.randn(3,4)\n",
        "print(a)\n",
        "\n",
        "print(\"#2\",end='\\n')\n",
        "print(torch.max(a))\n",
        "\n",
        "print(\"#3\",end='\\n')\n",
        "print(torch.max(a,dim=0))\n",
        "\n",
        "print(\"#4\",end='\\n')\n",
        "print(torch.max(a,dim=1))\n",
        "\n",
        "print(\"#5\",end='\\n')\n",
        "print(torch.max(a,dim=0,keepdims=True))\n",
        "\n",
        "print(\"#6\",end='\\n')\n",
        "print(torch.max(a,dim=1,keepdims=True)) #3*1짜리 2D tensor\n",
        "\n",
        "print(\"#7\",end='\\n')\n",
        "print(torch.argmax(a))\n",
        "\n",
        "print(\"#8\",end='\\n')\n",
        "print(torch.argmax(a,dim=0))\n",
        "\n",
        "print(\"#9\",end='\\n')\n",
        "print(torch.argmax(a,dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG0pPYe6h4-4",
        "outputId": "c6b09ecc-c3be-47a2-daef-ca020946580e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#1\n",
            "tensor([[ 0.2755, -0.9994, -0.4672,  0.3703],\n",
            "        [ 0.2186, -0.4692, -0.2131, -1.4987],\n",
            "        [ 1.5308,  1.0979, -0.4507, -0.8719]])\n",
            "#2\n",
            "tensor(1.5308)\n",
            "#3\n",
            "torch.return_types.max(\n",
            "values=tensor([ 1.5308,  1.0979, -0.2131,  0.3703]),\n",
            "indices=tensor([2, 2, 1, 0]))\n",
            "#4\n",
            "torch.return_types.max(\n",
            "values=tensor([0.3703, 0.2186, 1.5308]),\n",
            "indices=tensor([3, 0, 0]))\n",
            "#5\n",
            "torch.return_types.max(\n",
            "values=tensor([[ 1.5308,  1.0979, -0.2131,  0.3703]]),\n",
            "indices=tensor([[2, 2, 1, 0]]))\n",
            "#6\n",
            "torch.return_types.max(\n",
            "values=tensor([[0.3703],\n",
            "        [0.2186],\n",
            "        [1.5308]]),\n",
            "indices=tensor([[3],\n",
            "        [0],\n",
            "        [0]]))\n",
            "#7\n",
            "tensor(8)\n",
            "#8\n",
            "tensor([2, 2, 1, 0])\n",
            "#9\n",
            "tensor([3, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.randn(6,1)\n",
        "print(a)\n",
        "print(\"======torch.sort=======\") # torch라이브러리에서 sort 함수 사용\n",
        "a_sorted = torch.sort(a,dim=0,descending=True)\n",
        "print(a_sorted)\n",
        "\n",
        "print(\"======a.sort=======\") # a 인스턴스에서 sort 함수 사용\n",
        "a_sorted = a.sort(dim=0,descending=True)\n",
        "print(a_sorted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtjdKWNqoXl6",
        "outputId": "99979a71-42a1-48d1-c805-6b0904698607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.6082],\n",
            "        [-0.2584],\n",
            "        [ 0.1157],\n",
            "        [-0.0025],\n",
            "        [-0.8025],\n",
            "        [-1.5824]])\n",
            "======torch.sort=======\n",
            "torch.return_types.sort(\n",
            "values=tensor([[ 0.6082],\n",
            "        [ 0.1157],\n",
            "        [-0.0025],\n",
            "        [-0.2584],\n",
            "        [-0.8025],\n",
            "        [-1.5824]]),\n",
            "indices=tensor([[0],\n",
            "        [2],\n",
            "        [3],\n",
            "        [1],\n",
            "        [4],\n",
            "        [5]]))\n",
            "======a.sort=======\n",
            "torch.return_types.sort(\n",
            "values=tensor([[ 0.6082],\n",
            "        [ 0.1157],\n",
            "        [-0.0025],\n",
            "        [-0.2584],\n",
            "        [-0.8025],\n",
            "        [-1.5824]]),\n",
            "indices=tensor([[0],\n",
            "        [2],\n",
            "        [3],\n",
            "        [1],\n",
            "        [4],\n",
            "        [5]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.max(a))\n",
        "print(a.max())\n",
        "\n",
        "print(torch.abs(a))\n",
        "print(a.abs())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zqfANogorMa",
        "outputId": "05354bf2-18f2-433a-ef13-c3408780f930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6082)\n",
            "tensor(0.6082)\n",
            "tensor([[0.6082],\n",
            "        [0.2584],\n",
            "        [0.1157],\n",
            "        [0.0025],\n",
            "        [0.8025],\n",
            "        [1.5824]])\n",
            "tensor([[0.6082],\n",
            "        [0.2584],\n",
            "        [0.1157],\n",
            "        [0.0025],\n",
            "        [0.8025],\n",
            "        [1.5824]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.randn(3,4)\n",
        "print(a)\n",
        "print(\"==================\")\n",
        "print(a.sum(axis=0))\n",
        "print(a.sum(axis=0,keepdim=True))\n",
        "print(\"==================\")\n",
        "print(a.sum(axis=1))\n",
        "print(a.sum(axis=1,keepdim=True))\n",
        "print(\"==================\")\n",
        "print(torch.mean(a,axis=1,keepdim=True))\n",
        "print(a.mean(axis=1,keepdim=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNOzVNJFUHuX",
        "outputId": "c7c8de0d-5e9f-4892-8d28-9a6bf9efb216"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.7511, -1.1419, -0.9377, -0.6555],\n",
            "        [-2.7467,  0.0268, -1.2057,  0.4013],\n",
            "        [ 1.6963,  0.4609,  1.7138, -1.5849]])\n",
            "==================\n",
            "tensor([-1.8014, -0.6542, -0.4296, -1.8391])\n",
            "tensor([[-1.8014, -0.6542, -0.4296, -1.8391]])\n",
            "==================\n",
            "tensor([-3.4862, -3.5243,  2.2861])\n",
            "tensor([[-3.4862],\n",
            "        [-3.5243],\n",
            "        [ 2.2861]])\n",
            "==================\n",
            "tensor([[-0.8715],\n",
            "        [-0.8811],\n",
            "        [ 0.5715]])\n",
            "tensor([[-0.8715],\n",
            "        [-0.8811],\n",
            "        [ 0.5715]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape\n",
        "a= torch.randint(1,5,size=(12,))\n",
        "# 1부터 5미만의 정수 , 1차원은 (n, )과 같이 표현\n",
        "print(a)\n",
        "print(\"=======================\")\n",
        "print(a.reshape(3,4))\n",
        "print(a.reshape(3,-1)) # -1은 알아서 채워줘\n",
        "print(\"=======================\")\n",
        "print(a.reshape(2,2,3))\n",
        "print(a.reshape(2,2,3).ndim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQCGlwaOVRRN",
        "outputId": "ab988b8a-2437-42c9-ce3c-2f1c53ee7137"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3, 2, 4, 1, 1, 3, 2, 3, 3, 2, 2, 3])\n",
            "=======================\n",
            "tensor([[3, 2, 4, 1],\n",
            "        [1, 3, 2, 3],\n",
            "        [3, 2, 2, 3]])\n",
            "tensor([[3, 2, 4, 1],\n",
            "        [1, 3, 2, 3],\n",
            "        [3, 2, 2, 3]])\n",
            "=======================\n",
            "tensor([[[3, 2, 4],\n",
            "         [1, 1, 3]],\n",
            "\n",
            "        [[2, 3, 3],\n",
            "         [2, 2, 3]]])\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape의 응용\n",
        "a=torch.tensor([1,2,3])\n",
        "b=torch.tensor([2,2,1])\n",
        "# 내적 sol1)\n",
        "print(torch.sum(a*b))\n",
        "print(\"=================\")\n",
        "# 내적 sol2)\n",
        "# 벡터는 열벡터가 기본이므로 열벡터로 만들어주기\n",
        "a=a.reshape(3,1)\n",
        "b=b.reshape(3,1)\n",
        "print(a.T@b)\n",
        "print(a.t()@b)\n",
        "\n",
        "print(a.transpose(0,1)@b)\n",
        "print(a.permute(1,0)@b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xM4OrvhV8rY",
        "outputId": "accd5512-159d-4833-e69e-a2f38a0a1ec8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9)\n",
            "=================\n",
            "tensor([[9]])\n",
            "tensor([[9]])\n",
            "tensor([[9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.randn(4,3,6)\n",
        "print(a.shape)\n",
        "print(a.permute(2,0,1).shape)\n",
        "print(a.transpose(2,1).shape) # transpose는 둘끼리 자리 바꾸기만 가능"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxgRQafEdelI",
        "outputId": "ba44cf7a-81e4-4d1f-9e14-43a5e0ed705d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3, 6])\n",
            "torch.Size([6, 4, 3])\n",
            "torch.Size([4, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch: Reshape(View) vs Permute(Transpose) 완벽 정리\n",
        "\n",
        "2차원 행렬에서는 `reshape`과 `transpose`의 차이가 모호할 수 있으나, **3차원 이상에서는 데이터가 처리되는 방식이 완전히 다릅니다.**\n",
        "\n",
        "## 1. 핵심 요약 (한 줄 정리)\n",
        "\n",
        "| 구분 | 명령어 | 핵심 동작 원리 | 비유 |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Reshape** | `.reshape()`, `.view()` | 데이터를 **1줄로 쫙 편 다음(Flatten)**, 순서대로 다시 잘라서 채워 넣음. | **사과 상자:** 사과를 다 꺼내서 한 줄로 세운 뒤, 다른 크기의 상자에 순서대로 다시 담기. |\n",
        "| **Permute** | `.permute()`, `.transpose()` | 데이터 값은 그대로 두고, **축(Axis)의 순서(주소)**만 바꿈. | **책장 정리:** 책의 내용은 그대로인데, '과학 코너'와 '역사 코너'의 위치를 통째로 바꿈. |\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 3차원에서의 차이점 직관적 이해\n",
        "\n",
        "**(2, 2, 3)** 형태의 텐서를 **(3, 2, 2)**로 바꿀 때 어떤 일이 일어나는가?\n",
        "\n",
        "### A. Reshape (구조 파괴)\n",
        "> \"데이터의 물리적 순서를 유지하려다 보니, 의미가 섞인다.\"\n",
        "\n",
        "1.  $(2, 2, 3)$ 텐서를 메모리 상에서 1차원 배열(길이 12)로 풉니다.\n",
        "2.  앞에서부터 순서대로 잘라서 $(3, 2, 2)$ 틀에 억지로 끼워 넣습니다.\n",
        "3.  **결과:** $R, G, B$ 픽셀 정보가 뒤죽박죽 섞여버립니다. (이미지 노이즈 발생)\n",
        "\n",
        "### B. Permute (구조 유지)\n",
        "> \"데이터의 위치(인덱스) 규칙을 바꾼다.\"\n",
        "\n",
        "1.  데이터를 1차원으로 풀지 않습니다.\n",
        "2.  **인덱스(주소)의 순서**를 바꿉니다.\n",
        "    * 기존: `Tensor[i][j][k]`\n",
        "    * 변경: `Tensor[k][i][j]` (만약 `.permute(2, 0, 1)`을 했다면)\n",
        "3.  **결과:** $R$ 채널은 $R$끼리, $G$는 $G$끼리 묶인 채로 차원의 순서만 변경됩니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Python 코드 예시\n",
        "\n",
        "이 코드를 실행해보면 `reshape`이 데이터를 어떻게 섞어버리는지, `permute`가 어떻게 축을 교환하는지 명확히 알 수 있습니다.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# (2, 2, 3) 텐서 생성\n",
        "# 값의 의미: [0번째 덩어리, 1번째 행, 2번째 열] -> 012\n",
        "x = torch.tensor([\n",
        "    [[000, 001, 002],\n",
        "     [010, 011, 012]],\n",
        "    \n",
        "    [[100, 101, 102],\n",
        "     [110, 111, 112]]\n",
        "])\n",
        "\n",
        "print(f\"원본 Shape: {x.shape}\")\n",
        "# 출력: torch.Size([2, 2, 3])\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Case 1: Reshape (View) -> 의미 파괴\n",
        "# ---------------------------------------------------------\n",
        "# 12개의 숫자를 한 줄로 세운 뒤, 앞에서부터 끊어서 배치함\n",
        "x_reshape = x.reshape(3, 2, 2)\n",
        "\n",
        "print(\"\\n=== Reshape 결과 (의미 섞임) ===\")\n",
        "print(x_reshape)\n",
        "# 원본의 (0,0,2)에 있던 '002' 값이 뜬금없는 위치로 이동함.\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Case 2: Permute -> 의미 유지 (축 교환)\n",
        "# ---------------------------------------------------------\n",
        "# (0번축, 1번축, 2번축) 순서를 -> (2번축, 0번축, 1번축) 순서로 변경\n",
        "x_permute = x.permute(2, 0, 1)\n",
        "\n",
        "print(\"\\n=== Permute 결과 (의미 보존) ===\")\n",
        "print(x_permute)\n",
        "# 원본의 '012' 값(인덱스 0,1,2)은 여기서 인덱스 (2,0,1) 위치에서 발견됨.\n",
        "# 데이터가 섞이지 않고 차원만 회전함.\n",
        "```\n",
        "\n",
        "## 4. 실전 활용 (Computer Vision)\n",
        "\n",
        "CV 분야에서 이 개념은 필수입니다. 라이브러리마다 이미지 포맷이 다르기 때문입니다.\n",
        "\n",
        "* **PyTorch:** `(Channel, Height, Width)` $\\rightarrow$ `(C, H, W)`\n",
        "* **Matplotlib / OpenCV:** `(Height, Width, Channel)` $\\rightarrow$ `(H, W, C)`\n",
        "\n",
        "**이미지 시각화 할 때:**\n",
        "PyTorch 텐서를 `plt.imshow()`로 그리려면, `reshape`이 아니라 반드시 **`permute(1, 2, 0)`**를 써서 `(H, W, C)`로 바꿔줘야 정상적인 그림이 나옵니다."
      ],
      "metadata": {
        "id": "49DjWbR6biw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([\n",
        "    [[0, 1, 2],\n",
        "     [10, 11, 12]],\n",
        "\n",
        "    [[100, 101, 102],\n",
        "     [110, 111, 112]]\n",
        "])\n",
        "print(x)\n",
        "x = x.permute(1,2,0)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6rv0ckebwIw",
        "outputId": "b61c84b4-24c0-418e-813f-a8b35f4a26e2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[  0,   1,   2],\n",
            "         [ 10,  11,  12]],\n",
            "\n",
            "        [[100, 101, 102],\n",
            "         [110, 111, 112]]])\n",
            "tensor([[[  0, 100],\n",
            "         [  1, 101],\n",
            "         [  2, 102]],\n",
            "\n",
            "        [[ 10, 110],\n",
            "         [ 11, 111],\n",
            "         [ 12, 112]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# :와 ...\n",
        "x=torch.randn(2,3,4,5,6)\n",
        "print(x.shape)\n",
        "print(\"==================\")\n",
        "print(x[1,2,:,:,:].shape)\n",
        "print(x[1,2,...].shape) # 위와 같은 의미\n",
        "print(\"==================\")\n",
        "print(x[...,3].shape)\n",
        "print(x[1,1,...,2].shape)\n",
        "print(x[1,1,...,2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj4QxFi4bjjz",
        "outputId": "ccff7783-2ea6-4a76-90ae-379b81104689"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 4, 5, 6])\n",
            "==================\n",
            "torch.Size([4, 5, 6])\n",
            "torch.Size([4, 5, 6])\n",
            "==================\n",
            "torch.Size([2, 3, 4, 5])\n",
            "torch.Size([4, 5])\n",
            "tensor([[ 0.6908,  1.9506,  1.5392,  0.5119, -0.4191],\n",
            "        [-2.6318, -0.3726,  0.8262,  0.0895,  0.4527],\n",
            "        [ 0.1750,  0.3060, -0.3529, -0.2931,  0.8611],\n",
            "        [-0.3409, -0.2972, -2.1388,  1.0565,  1.0503]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.randn(2,3,4)\n",
        "print(a)\n",
        "print(a.shape)\n",
        "print(a[:,1,:])\n",
        "print(a[:,1,:].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5nFflWfg2oY",
        "outputId": "b07a9694-749d-4640-d48b-a8073768daf4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-3.0398,  1.0868,  0.4214, -0.7843],\n",
            "         [-0.5671,  0.5987,  0.9501, -0.5426],\n",
            "         [-0.8413,  0.0946, -1.4603,  1.2323]],\n",
            "\n",
            "        [[-0.1307,  0.3309, -0.8102,  0.7373],\n",
            "         [ 0.6615, -0.5942,  0.9128, -0.7104],\n",
            "         [ 0.6831,  1.6327, -0.7188,  1.4635]]])\n",
            "torch.Size([2, 3, 4])\n",
            "tensor([[-0.5671,  0.5987,  0.9501, -0.5426],\n",
            "        [ 0.6615, -0.5942,  0.9128, -0.7104]])\n",
            "torch.Size([2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenation\n",
        "a=torch.ones(3,4)\n",
        "b=torch.zeros(3,4)\n",
        "c=torch.vstack([a,b]) #v = 0dim요소들끼리\n",
        "d=torch.hstack([a,b]) #h = 1dim요소들끼리\n",
        "e=torch.cat([a,b],dim=0) #0dim요소들끼리\n",
        "f=torch.cat([a,b],dim=1) #1dim요소들끼리\n",
        "\n",
        "print(c)\n",
        "print(e)\n",
        "print(d)\n",
        "print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWbGkDaTg2VQ",
        "outputId": "3c4e2203-7bf4-4c84-8e11-f4ab10a89737"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "tensor([[1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.]])\n",
            "tensor([[1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# squeeze\n",
        "# 쓸모없는 1짜리 차원을 없애줌\n",
        "\n",
        "a = torch.randn(1,1,1,3,1,1,1,4,1,1,1)\n",
        "print(a.shape)\n",
        "print(a.squeeze().shape)\n",
        "print(a.squeeze(dim=[0,1,2]).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8sWqmT9jXyJ",
        "outputId": "8e570eb8-de5e-43bc-a8c1-e528afd121b4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 1, 3, 1, 1, 1, 4, 1, 1, 1])\n",
            "torch.Size([3, 4])\n",
            "torch.Size([3, 1, 1, 1, 4, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unsqueeze\n",
        "# 1짜리 차원을 늘려줌\n",
        "a=torch.randn(3,4)\n",
        "print(a.unsqueeze(0).shape)\n",
        "print(a.reshape(1,3,4).shape)\n",
        "\n",
        "print(a.unsqueeze(1).shape)\n",
        "print(a.reshape(3,1,4).shape)\n",
        "\n",
        "print(a.unsqueeze(2).shape)\n",
        "print(a.reshape(3,4,1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4PWoxC_jx7q",
        "outputId": "12909384-8782-42e8-d8d8-d4f4e9ae17ee"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 4])\n",
            "torch.Size([1, 3, 4])\n",
            "torch.Size([3, 1, 4])\n",
            "torch.Size([3, 1, 4])\n",
            "torch.Size([3, 4, 1])\n",
            "torch.Size([3, 4, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unsqueeze의 유용한 활용처\n",
        "a=torch.ones(3,4)\n",
        "b=torch.zeros(3,4)\n",
        "print(a)\n",
        "print(b)\n",
        "print(\"==========================\")\n",
        "aa=a.unsqueeze(0)\n",
        "bb=b.unsqueeze(0)\n",
        "print(aa)\n",
        "print(bb)\n",
        "print(\"==========================\")\n",
        "c=torch.cat([aa,bb],axis=0)\n",
        "print(c)\n",
        "print(\"==========================\")\n",
        "d=torch.stack([a,b],axis=0)\n",
        "\n",
        "# 차원을 추가해서 쌓는 방식이다보니,\n",
        "# stack은 a,b의 shape이 완전히 일치해야함.\n",
        "# concat은 달라도 쌓을 차원 이외의 차원들만 일치하면 됨."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcFMxMNXkSzL",
        "outputId": "e3e67202-4606-4bde-d4ff-f012aec329b1"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "==========================\n",
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n",
            "tensor([[[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]])\n",
            "==========================\n",
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]])\n",
            "==========================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ② `cat` vs `stack` 비교\n",
        "\n",
        "두 방식의 결과물은 완전히 동일합니다.\n",
        "\n",
        "1.  **`unsqueeze` + `cat` 방식 (수동 구현)**\n",
        "    * 각각 3차원으로 만든 뒤(`1, 3, 4`), 0번 축을 따라 이어 붙입니다.\n",
        "    * 결과: `(1+1, 3, 4)` = `(2, 3, 4)`\n",
        "2.  **`stack` 방식 (자동 구현)**\n",
        "    * 2차원 데이터(`3, 4`)를 새로운 차원을 생성하며 쌓아 올립니다.\n",
        "    * 결과: `(2, 3, 4)`\n",
        "\n",
        "> **결론:** `torch.stack`은 사용자의 편의를 위해 `unsqueeze`와 `cat` 과정을 한 번에 처리해 주는 함수입니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 실전 활용: `unsqueeze`는 언제 쓰는가?\n",
        "\n",
        "AI 모델링(특히 CV, NLP)에서 `unsqueeze`는 필수적으로 사용됩니다. 가장 대표적인 사례는 **\"배치 차원(Batch Dimension) 추가\"**입니다.\n",
        "\n",
        "### 상황: 이미지 1장을 모델에 넣을 때\n",
        "PyTorch의 모델(CNN, ResNet 등)은 입력 데이터가 **반드시 4차원**이어야 합니다.\n",
        "\n",
        "* **모델이 기대하는 형태:** `(Batch, Channel, Height, Width)`\n",
        "* **내가 가진 이미지:** `(Channel, Height, Width)`\n",
        "\n",
        "이때 `unsqueeze`를 사용하여 가짜 차원(Batch=1)을 만들어줍니다.\n",
        "\n",
        "```python\n",
        "# 내 이미지 (3채널, 224x224)\n",
        "my_image = torch.randn(3, 224, 224)\n",
        "\n",
        "# [Error 발생] 모델은 4차원을 원함\n",
        "# model(my_image)\n",
        "\n",
        "# [해결] 0번 축(맨 앞)에 차원 추가 -> (1, 3, 224, 224)\n",
        "input_batch = my_image.unsqueeze(0)\n",
        "\n",
        "# [성공] 모델이 \"아, 1장짜리 묶음이구나\" 하고 인식함\n",
        "output = model(input_batch)\n",
        "```\n",
        "\n",
        "## 3. 요약\n",
        "\n",
        "| 함수 | 역할 | 비유 |\n",
        "| :--- | :--- | :--- |\n",
        "| **`unsqueeze(dim)`** | 해당 위치에 크기 1인 차원 생성 | 낱장을 파일 홀더에 넣기 |\n",
        "| **`cat`** | 기존 차원을 따라 연결 | 기차 연결하기 |\n",
        "| **`stack`** | 새로운 차원을 만들어 쌓기 | 샌드위치 쌓기 |"
      ],
      "metadata": {
        "id": "cl8rU4IQn1vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 딥러닝 기초: 배치(Batch)란 무엇인가?\n",
        "\n",
        "딥러닝 입문자가 가장 많이 헷갈리는 '차원' 개념, 그중에서도 **배치(Batch)**에 대한 핵심 정리입니다.\n",
        "\n",
        "## 1. 직관적 이해: \"한 번에 처리하는 묶음\"\n",
        "\n",
        "**비유: 군대 식판 닦기**\n",
        "* **Batch 안 쓸 때 (Batch Size = 1):**\n",
        "    취사병이 훈련병 1명이 밥 다 먹을 때까지 기다렸다가, 식판 1개를 닦고, 다음 사람을 기다림. $\\rightarrow$ **매우 느리고 비효율적**\n",
        "* **Batch 쓸 때 (Batch Size = 32):**\n",
        "    식판 32개가 모일 때까지 기다렸다가, **한 번에** 32개를 세척기에 넣고 돌림. $\\rightarrow$ **고효율 (병렬 처리)**\n",
        "\n",
        "> **Batch Size:** 한 번에 묶어서 처리하는 데이터의 개수 (예: 16, 32, 64, 128...)\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 왜 사용하는가? (GPU의 특성)\n",
        "\n",
        "컴퓨터(GPU)는 거대한 공장 기계와 같습니다. 기계를 한 번 돌릴 때마다 비용(연산)이 발생하는데, 고작 데이터 1개만 넣으면 손해입니다.\n",
        "그래서 AI 모델은 설계될 때부터 **\"데이터를 묶음(Batch) 단위로 넣어라\"**라고 규칙이 정해져 있습니다.\n",
        "\n",
        "## 3. 차원으로 보는 Batch (3차원 → 4차원)\n",
        "\n",
        "이미지 데이터를 예로 들면, 배치가 추가됨으로써 맨 앞에 차원이 하나 늘어납니다.\n",
        "\n",
        "* **이미지 1장 (Raw Data):** `(Channel, Height, Width)`\n",
        "* **배치 (Input Data):** `(Batch_Size, Channel, Height, Width)`\n",
        "\n",
        "### 차원 변화 예시\n",
        "* **사과 1개:** `(3, 100, 100)`\n",
        "* **사과 32개 묶음:** `(32, 3, 100, 100)`\n",
        "* **맨 앞의 숫자(N)**가 바로 묶음의 크기입니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. `unsqueeze(0)`가 필요한 이유\n",
        "\n",
        "우리가 실습할 때 이미지 1장만 가지고 있더라도, 모델(기계)은 **\"무조건 묶음 단위\"**만 받습니다. 사과 1개를 그냥 투입구에 던지면 \"형식이 맞지 않습니다\"라며 에러가 납니다.\n",
        "\n",
        "따라서 **\"이것은 1개짜리 묶음입니다\"**라고 포장(차원 추가)을 해줘야 합니다.\n",
        "\n",
        "```python\n",
        "# 내 손에 있는 것 (사과 1개)\n",
        "img = torch.randn(3, 100, 100)\n",
        "# -> 기계에 넣으면 \"차원 부족\" 에러 발생\n",
        "\n",
        "# 포장지 씌우기 (1개짜리 박스 만들기)\n",
        "img_batch = img.unsqueeze(0)\n",
        "\n",
        "# -> Shape 결과: (1, 3, 100, 100)\n",
        "# -> 기계의 반응: \"아, 1개짜리 묶음(Batch Size=1)이구나. 통과!\"\n",
        "```\n",
        "\n",
        "## 요약\n",
        "1.  **Batch**: 효율성을 위해 데이터를 묶어서 처리하는 단위.\n",
        "2.  **Model**: 무조건 Batch 차원(맨 앞, 0번 축)이 포함된 4차원 입력을 원함.\n",
        "3.  **unsqueeze(0)**: 데이터가 1개일 때, 강제로 배치 차원을 만들어주기 위해(포장하기 위해) 사용."
      ],
      "metadata": {
        "id": "UTrIqMpdojU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone의 필요성\n",
        "a = torch.tensor([[1,2],[3,4]])\n",
        "b = a     # 주소까지 가져와서 일심동체\n",
        "b[0,0]=10000\n",
        "print(a)  # a도 바뀜"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-XM0bFKn2Pw",
        "outputId": "e66be786-c458-4691-9bdb-d5ce707e64ae"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[10000,     2],\n",
            "        [    3,     4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone 사용\n",
        "a = torch.tensor([[1,2],[3,4]])\n",
        "b = a.clone()\n",
        "b[0,0]=10000\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JckJTWHIpu5L",
        "outputId": "730a37dc-c9b1-402e-bdd8-50bc2b8a72f1"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch: 행렬 곱(@)의 3가지 동작 방식\n",
        "\n",
        "`@` 연산자(`torch.matmul`)는 입력된 텐서의 차원에 따라 '스마트하게' 동작합니다. 이 규칙은 Transformer나 CNN 내부 연산의 기초가 됩니다.\n",
        "\n",
        "## 1. 차원별 동작 원리\n",
        "\n",
        "| 케이스 | 입력 A Shape | 입력 B Shape | 결과 Shape | 동작 설명 |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **기본 (2D)** | $(5, \\mathbf{7})$ | $(\\mathbf{7}, 10)$ | $(5, 10)$ | 일반적인 행렬 곱셈. 안쪽 차원($7$) 소거. |\n",
        "| **배치 (3D)** | $(\\mathbf{32}, 5, 7)$ | $(\\mathbf{32}, 7, 10)$ | $(\\mathbf{32}, 5, 10)$ | **끼리끼리 연산.** 배치($32$)별로 독립적으로 행렬 곱 수행. |\n",
        "| **브로드캐스팅** | $(\\mathbf{32}, 5, 7)$ | $(7, 10)$ | $(\\mathbf{32}, 5, 10)$ | **공유 연산.** 하나의 $B$ 행렬이 $A$의 모든 배치($32$)에 동일하게 적용됨. |\n",
        "\n",
        "## 2. 코드 예시 및 해석\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# 1. Standard MatMul (단일 연산)\n",
        "a = torch.randn(5, 7)\n",
        "b = torch.randn(7, 10)\n",
        "print( (a@b).shape )\n",
        "# >> torch.Size([5, 10])\n",
        "\n",
        "# 2. Batch MatMul (개별 연산)\n",
        "# 상황: 32개의 데이터가 각각 서로 다른 변환 행렬과 곱해질 때\n",
        "a = torch.randn(32, 5, 7)\n",
        "b = torch.randn(32, 7, 10)\n",
        "print( (a@b).shape )\n",
        "# >> torch.Size([32, 5, 10])\n",
        "\n",
        "# 3. Broadcasting (공유 연산) - 중요!\n",
        "# 상황: 32개의 데이터(Batch)가 하나의 가중치(Weight)를 공유할 때 (예: nn.Linear)\n",
        "a = torch.randn(32, 5, 7)\n",
        "b = torch.randn(7, 10)    # 배치 차원 없음\n",
        "print( (a@b).shape )\n",
        "# >> torch.Size([32, 5, 10])\n",
        "# b가 자동으로 (32, 7, 10)인 것처럼 행동함\n",
        "```\n",
        "\n",
        "> **Tip:** `@` 연산 시 `RuntimeError`가 발생한다면 99% 확률로 **\"안쪽 차원(Inner Dimension)\"**이 맞지 않아서입니다. (위 예시에서는 `7`에 해당)"
      ],
      "metadata": {
        "id": "TxnyAtMvsL8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @ 내적\n",
        "a = torch.randn(5,7)\n",
        "b = torch.randn(7,10)\n",
        "c = a@b\n",
        "print(c.shape)\n",
        "\n",
        "a = torch.randn(32,5,7)\n",
        "b = torch.randn(32,7,10)\n",
        "c = a@b\n",
        "print(c.shape)\n",
        "\n",
        "a = torch.randn(32,5,7)\n",
        "b = torch.randn(7,10)\n",
        "c = a@b\n",
        "print(c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R6sdxIMqAC5",
        "outputId": "d28d75c8-d3c4-4980-af59-7907e2d46821"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 10])\n",
            "torch.Size([32, 5, 10])\n",
            "torch.Size([32, 5, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy <-> torch 왔다갔다 기능\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "a = np.array([1,2,3])\n",
        "b = torch.tensor([1,2,3])\n",
        "A = torch.tensor(a)  # A=torch.from_numpy(a)\n",
        "B = np.array(b) # B=b.numpy()\n",
        "print(type(A))\n",
        "print(type(B))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg9iLPGEpzsF",
        "outputId": "8e76bab5-3839-440a-8410-aab74c5ab612"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4kiwqFA9s9a_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}